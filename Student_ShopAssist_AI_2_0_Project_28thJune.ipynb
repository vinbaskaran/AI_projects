{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinbaskaran/AI_projects/blob/main/Student_ShopAssist_AI_2_0_Project_28thJune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lget6YStkk-N"
      },
      "source": [
        "## ShopAssist AI 2.0\n",
        "\n",
        "In this sample solution, we have provided a method to combine function calling API with the ShopAssist AI 2.0's system design to make it more seamless, and with less lines of code!\n",
        "\n",
        "This solution, redefines our system design, and refines the overall conversation flow. Also, the output of the layers are now more consistent and therefore, require less manipulation for a seamless conversation.\n",
        "\n",
        "Note that this is an outline solution, and hence contains primarily the function calling API prototype only. You are suggested to modify this further, and add additional layers (moderation, etc.) to modify the output as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4ukINBvyvQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5026249-3894-427b-cc6b-62724cefc4a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.6.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.20.1)\n"
          ]
        }
      ],
      "source": [
        "#Install openai\n",
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJJVMuodyb2u"
      },
      "outputs": [],
      "source": [
        "## Import the necessary libraries for building the project\n",
        "import os, json, ast\n",
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "openai.api_key = userdata.get('api_key_s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GesnkvbCzJLS"
      },
      "outputs": [],
      "source": [
        "## Function Description for the Function Calling API\n",
        "\n",
        "function_descriptions = [\n",
        "            {\n",
        "                \"name\": \"compare_laptops_with_user\",\n",
        "                \"description\": \"Get the top 3 laptops from the catalogue, that best matches what the user is asking based on 'GPU intensity','Display quality','Portability','Multitasking','Processing speed' & 'Budget\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"gpu intensity\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The requirement of the user in GPU capacity classfied as low, medium or high\" ,\n",
        "                        },\n",
        "                        \"display quality\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The requirement of the user for Laptop's Display Quality & capacity classfied as low, medium or high\" ,\n",
        "                        },\n",
        "                        \"portability\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The requirement of the user for Laptop's portability classfied as low, medium or high\" ,\n",
        "                        },\n",
        "                        \"multitasking\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The requirement of the user for Laptop's Multitasking classfied as low, medium or high\" ,\n",
        "                        },\n",
        "                        \"processing speed\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The requirement of the user for Laptop's Processing speed classfied as low, medium or high\" ,\n",
        "                        },\n",
        "                        \"budget\": {\n",
        "                            \"type\": \"integer\",\n",
        "                            \"description\": \"The maximum budget of the user\" ,\n",
        "                        },\n",
        "\n",
        "                    },\n",
        "                    \"required\": [\"GPU intensity\",\"Display quality\",\"Portability\",\"Multitasking\",\"Processing speed\",\"Budget\"],\n",
        "                },\n",
        "            }\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZtYd9Sj6PJP"
      },
      "outputs": [],
      "source": [
        "def initialize_conversation():\n",
        "    delimiter = \"####\"\n",
        "\n",
        "    system_message = f\"\"\"\n",
        "    You are an intelligent laptop gadget expert and your goal is to find the best laptop for a user.\n",
        "    You are trying to understand the user's requirement for these laptop's features: ('gpu intensity','display quality','portability','multitasking','processing speed','budget')\n",
        "    You need to ask relevant questions and understand the user need for each feature by analysing the user's responses.\n",
        "    After understanding their requirements, you'll use a function call to suggest the top 3 laptops with their respective user match score.\n",
        "    Recommend these laptops and answer any user's query about them.\n",
        "\n",
        "    {delimiter} Here are certain guidelines that you need to follow:\n",
        "    Don't ask questions about more than 2 features at a time.\n",
        "    If the user's budget is less than says less than 25000 INR, please mention that there are no laptops in that range.\n",
        "    Recommend the top3 laptops in the following format:\n",
        "    Start with a brief summary of each laptop in the following format, in decreasing order of price of laptops:\n",
        "    1. <Laptop Name> : <Major specifications of the laptop>, <Price in Rs>\n",
        "    2. <Laptop Name> : <Major specifications of the laptop>, <Price in Rs>\n",
        "    3. <Laptop Name> : <Major specifications of the laptop>, <Price in Rs>\n",
        "    {delimiter}\n",
        "\n",
        "    {delimiter}To find the top3 laptops, you need to have the following chain of thoughts:\n",
        "    Thought 1: Ask one question to understand the user's profile and requirements. \\n\n",
        "    If their primary use for the laptop is unclear. Ask another question to comprehend their needs.\n",
        "    Answer \"Yes\" or \"No\" to indicate if you understand the requirements. \\n\n",
        "    If yes, proceed to the next step. Otherwise, rephrase the question to capture their profile. \\n\n",
        "\n",
        "    Thought 2: Now, you are trying to understand the requirements for other features which you couldn't in the previous step.\n",
        "    Ask questions to strengthen your understanding of the user's profile.\n",
        "    Don't ask questions about more than 2 features at a time.\n",
        "    Answer \"Yes\" or \"No\" to indicate if you understood all the needs of the features and are confident about the same.\n",
        "    If yes, move to the next Thought. If no, ask question on the features whose needs you are unsure of. \\n\n",
        "    It is a good practice to ask question with a sound logic as opposed to directly citing the feature you want to understand the need for.{delimiter}\n",
        "    {delimiter}\n",
        "\n",
        "    {delimiter} Here is a sample conversation between the user and assistant:\n",
        "    Assistant: \"Hello! I'm here to help you find the perfect laptop that suits your needs. Could you please share your requirements?\"\n",
        "    User: \"Hi, I am an editor.\"\n",
        "    Assistant: \"Great! As an editor, you likely require a laptop that can handle demanding tasks. Hence, the laptop should have high multitasking capability. You would also need a high end display for better visuals and editing. May I know what kind of work do you primarily focus on? Are you more involved in video editing, photo editing, or both? Understanding the specific type of editing work will help me tailor my recommendations accordingly. Let me know if my understanding is correct until now.\"\n",
        "    User: \"I primarily work with After Effects.\"\n",
        "    Assistant: \"Thank you for providing that information. Working with After Effects involves working with graphics, animations, and rendering, which will require high GPU. Do you work with high-resolution media files, such as 4K videos or RAW photos? Understanding your file sizes will help determine the storage capacity and processing power needed.\"\n",
        "    User: \"Yes, sometimes I work with 4K videos as well.\"\n",
        "    Assistant: \"Thank you for the information. Processing 4K vidoes will require a good processor and high GPU. I think we have already determined earlier that you need a high GPU. To ensure I have a complete understanding of your needs, I have one more question: Are you frequently on the go and require a laptop that is lightweight and easy to carry, or do you primarily work from a stationary location?\"\n",
        "    User: \"Yes, sometimes I travel but do not carry my laptop.\"\n",
        "    Assistant:\"Could you kindly let me know your budget for the laptop? This will help me find options that fit within your price range while meeting the specified requirements.\"\n",
        "    User: \"my max budget is 1.5lakh inr\"\n",
        "    {delimiter}\n",
        "\n",
        "\n",
        "    Start with a short welcome message and encourage the user to share their requirements.\n",
        "    \"\"\"\n",
        "    conversation = [{\"role\": \"system\", \"content\": system_message }]\n",
        "    return conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFVCMMSz6GKj"
      },
      "outputs": [],
      "source": [
        "def get_chat_model_completions(messages):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model = 'gpt-4o',\n",
        "        messages = messages,\n",
        "        temperature = 0,\n",
        "        max_tokens = 500,\n",
        "\n",
        "        functions= function_descriptions,\n",
        "        function_call = \"auto\"\n",
        "\n",
        "    )\n",
        "    return response.choices[0].message"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Inut & Output"
      ],
      "metadata": {
        "id": "3_EFYCnzXdqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "laptop_recommendation = '[{\"Laptop Name\": \"Laptop A\", \"Price\": 75000, \"Score\": 5},\n",
        "                          {\"Laptop Name\": \"Laptop B\", \"Price\": 79000, \"Score\": 4},\n",
        "                          {\"Laptop Name\": \"Laptop C\", \"Price\": 78000, \"Score\": 2}]'\n",
        "\n",
        "recommendation_validation(laptop_recommendation)\n"
      ],
      "metadata": {
        "id": "C7PxDYsgXgik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\"gpu\":\"high\", \"portability\":\"medium\"}"
      ],
      "metadata": {
        "id": "s9JqDifsDJXn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W13nBUM_nv-M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "\n",
        "def extract_dictionary_from_string(string):\n",
        "    regex_pattern = r\"\\{[^{}]+\\}\"\n",
        "\n",
        "    dictionary_matches = re.findall(regex_pattern, string)\n",
        "\n",
        "    # Extract the first dictionary match and convert it to lowercase\n",
        "    if dictionary_matches:\n",
        "        dictionary_string = dictionary_matches[0]\n",
        "        dictionary_string = dictionary_string.lower()\n",
        "\n",
        "        # Convert the dictionary string to a dictionary object using ast.literal_eval()\n",
        "        dictionary = ast.literal_eval(dictionary_string)\n",
        "    return dictionary\n",
        "\n",
        "def compare_laptops_with_user(user_req_string):\n",
        "  budget = user_req_string.get('budget', '0')\n",
        "  laptop_df= pd.read_csv('updated_laptop.csv')\n",
        "  filtered_laptops = laptop_df.copy()\n",
        "  filtered_laptops['Price'] = filtered_laptops['Price'].str.replace(',','').astype(int)\n",
        "  filtered_laptops = filtered_laptops[filtered_laptops['Price'] <= budget].copy()\n",
        "\n",
        "  mappings = {\n",
        "          'low': 0,\n",
        "          'medium': 1,\n",
        "          'high': 2\n",
        "      }\n",
        "\n",
        "  # Create 'Score' column in the DataFrame and initialize to 0\n",
        "  filtered_laptops['Score'] = 0\n",
        "  for index, row in filtered_laptops.iterrows():\n",
        "      user_product_match_str = row['laptop_feature']\n",
        "      laptop_values = extract_dictionary_from_string(user_product_match_str)\n",
        "      score = 0\n",
        "\n",
        "      for key, user_value in user_req_string.items():\n",
        "        if key.lower() == 'budget':\n",
        "            continue  # Skip budget comparison\n",
        "        laptop_value = laptop_values.get(key, None)\n",
        "        laptop_mapping = mappings.get(laptop_value.lower(), -1)\n",
        "        user_mapping = mappings.get(user_value.lower(), -1)\n",
        "        if laptop_mapping >= user_mapping:\n",
        "          ### If the laptop value is greater than or equal to the user value the score is incremented by 1\n",
        "          score += 1\n",
        "\n",
        "      filtered_laptops.loc[index, 'Score'] = score\n",
        "\n",
        "  # Sort the laptops by score in descending order and return the top 5 products\n",
        "  top_laptops = filtered_laptops.drop('laptop_feature', axis=1)\n",
        "  top_laptops = top_laptops.sort_values('Score', ascending=False).head(3)\n",
        "\n",
        "  return top_laptops.to_json(orient='records')\n",
        "\n",
        "def recommendation_validation(laptop_recommendation):\n",
        "\n",
        "    data = json.loads(laptop_recommendation)\n",
        "    data1 = []\n",
        "    for i in range(len(data)):\n",
        "      if data[i]['Score'] > 2:\n",
        "        data1.append(data[i])\n",
        "\n",
        "    return json.dumps(data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZVtOt4Cmuuw"
      },
      "outputs": [],
      "source": [
        "def dialogue_mgmt_system():\n",
        "    conversation = initialize_conversation()\n",
        "\n",
        "    introduction = get_chat_model_completions(conversation)[\"content\"]\n",
        "    print(introduction + '\\n')\n",
        "    user_input = \"\"\n",
        "\n",
        "    while(user_input != \"exit\"):\n",
        "      user_input = input(\"\")\n",
        "      conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "\n",
        "      # Step 1: Pass the user input to GPT\n",
        "      response_assistant = get_chat_model_completions(conversation)\n",
        "\n",
        "      if response_assistant.get(\"function_call\"):\n",
        "        print(\"\\nThank you for providing all the information. Kindly wait, while I fetch the products\\n\")\n",
        "\n",
        "\n",
        "        # Step 2: Extract top3 laptops by calling the external function\n",
        "        function_name = response_assistant[\"function_call\"][\"name\"]\n",
        "\n",
        "        function_args = json.loads(response_assistant[\"function_call\"][\"arguments\"])\n",
        "        top_3_laptops = compare_laptops_with_user(function_args)\n",
        "        function_response = recommendation_validation(top_3_laptops)\n",
        "\n",
        "\n",
        "        if len(function_response) == 0:\n",
        "          print(\"Sorry, we do not have laptops that match your requirements. Connecting you to a human expert.\")\n",
        "          break\n",
        "\n",
        "        # Step 3: send the info on the function call and function response to GPT\n",
        "        conversation.append(response_assistant)\n",
        "        conversation.append(\n",
        "            {\n",
        "                \"role\": \"function\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": function_response,\n",
        "            }\n",
        "        )\n",
        "        recommendation = get_chat_model_completions(conversation)\n",
        "        conversation.append({\"role\": \"assistant\", \"content\": recommendation[\"content\"]})\n",
        "        print(\"\\n\" +recommendation[\"content\"] + \"\\n\")\n",
        "      else:\n",
        "        conversation.append({\"role\": \"assistant\", \"content\": response_assistant[\"content\"]})\n",
        "        print(\"\\n\" +  response_assistant[\"content\"] + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dialogue_mgmt_system()"
      ],
      "metadata": {
        "id": "xWzpKQLhNrul",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "2e57f1e5-4a2e-4a5d-a9fb-09e6495bd61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm here to help you find the perfect laptop that suits your needs. Could you please share your requirements or let me know what you'll primarily use the laptop for?\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-3339482843.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdialogue_mgmt_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-15-1526689453.py\u001b[0m in \u001b[0;36mdialogue_mgmt_system\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;31m# Step 1: Pass the user input to GPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am looking for a gaming laptop under 80,000 with an high end GPU and meddium diplay configuration"
      ],
      "metadata": {
        "id": "MVe8iw_aIy34"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}