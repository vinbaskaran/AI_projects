{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e6895499",
      "metadata": {
        "id": "e6895499"
      },
      "source": [
        "# Minimal RAG Application with LlamaIndex (Colab)\n",
        "\n",
        "This notebook helps you build a minimal Retrieval-Augmented Generation (RAG) app using LlamaIndex and OpenAI to answer questions about your insurance PDF.\n",
        "\n",
        "**Instructions:**\n",
        "- Upload your `OpenAI_API_Key.txt` and insurance PDF file (e.g., `Principal-Sample-Life-Insurance-Policy.pdf`) using the Colab file upload cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5626c418",
      "metadata": {
        "id": "5626c418",
        "outputId": "141345c1-627a-4f29-bad3-8daa277b2a54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install llama-index openai pdfplumber --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a620d1ac",
      "metadata": {
        "id": "a620d1ac"
      },
      "outputs": [],
      "source": [
        "# Read OpenAI API key and PDF filename\n",
        "import os\n",
        "\n",
        "api_key_path = 'OpenAI_API_Key.txt'\n",
        "pdf_path = 'Principal-Sample-Life-Insurance-Policy.pdf'\n",
        "\n",
        "with open(api_key_path, 'r') as f:\n",
        "    openai_api_key = f.read().strip()\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "44ea9d41",
      "metadata": {
        "id": "44ea9d41"
      },
      "outputs": [],
      "source": [
        "# Load and index the PDF using LlamaIndex\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# Load document\n",
        "reader = SimpleDirectoryReader(input_files=[pdf_path])\n",
        "documents = reader.load_data()\n",
        "\n",
        "# Set up LlamaIndex with OpenAI\n",
        "llm = OpenAI(model='gpt-3.5-turbo', api_key=openai_api_key)\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "048a49a1",
      "metadata": {
        "id": "048a49a1",
        "outputId": "ed81d0bf-2fca-4d94-bc4d-ca40327b9706",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question about the insurance policy: What happens at policy maturity?\n",
            "Answer: At policy maturity, the premium amount to be paid on each due date will be determined based on the total volume of insurance in force divided by 1,000, multiplied by the premium rate then in effect for Member Life Insurance, Member Accidental Death and Dismemberment Insurance, and Dependent Life Insurance. Additionally, if the Policyholder has other group insurance with The Principal and life coverage is added on a date other than the Policy Anniversary more than six months before the next Policy Anniversary, the premium rate may be changed on the next Policy Anniversary with written notice provided at least 31 days before the date of change.\n"
          ]
        }
      ],
      "source": [
        "# Ask questions about the PDF\n",
        "question = input('Enter your question about the insurance policy: ')\n",
        "response = query_engine.query(question)\n",
        "print('Answer:', response.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Application v2: Essential Improvements\n",
        "\n",
        "This version introduces modular workflow, advanced chunking, conversational memory, source attribution, and a better user interface for insurance PDF Q&A."
      ],
      "metadata": {
        "id": "enZIHUA2V_ai"
      },
      "id": "enZIHUA2V_ai"
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced chunking and metadata extraction\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# Use sentence splitter for finer chunking\n",
        "parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# Add metadata (e.g., page number) to each node\n",
        "for node in nodes:\n",
        "    if hasattr(node, 'metadata') and hasattr(node, 'text'):  # Defensive check\n",
        "        node.metadata['source'] = node.metadata.get('page_label', 'Unknown')"
      ],
      "metadata": {
        "id": "J5Lb5zA5USLX"
      },
      "id": "J5Lb5zA5USLX",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build index from advanced nodes\n",
        "index_v2 = VectorStoreIndex(nodes)\n",
        "query_engine_v2 = index_v2.as_query_engine(llm=llm)"
      ],
      "metadata": {
        "id": "FEcO7Zm0WCAA"
      },
      "id": "FEcO7Zm0WCAA",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive chat with conversational memory, exit, and clear commands\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "question_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type your question about the insurance policy...',\n",
        "    description='Question:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "output_box = widgets.Output()\n",
        "\n",
        "def on_submit(sender):\n",
        "    question = question_box.value.strip()\n",
        "    if not question:\n",
        "        return\n",
        "    if question.lower() == 'exit':\n",
        "        question_box.disabled = True\n",
        "        with output_box:\n",
        "            display(Markdown(\"**Chat ended. Refresh the notebook to start again.**\"))\n",
        "        return\n",
        "    if question.lower() == 'clear':\n",
        "        chat_history.clear()\n",
        "        output_box.clear_output()\n",
        "        question_box.value = ''\n",
        "        return\n",
        "    chat_history.append({'role': 'user', 'content': question})\n",
        "    response = query_engine.query(question)\n",
        "    chat_history.append({'role': 'assistant', 'content': response.response})\n",
        "    with output_box:\n",
        "        display(Markdown(f\"**Q:** {question}\"))\n",
        "        display(Markdown(f\"**A:** {response.response}\"))\n",
        "        # Show source if available\n",
        "        if hasattr(response, 'source_nodes') and response.source_nodes:\n",
        "            for node in response.source_nodes:\n",
        "                src = node.node.metadata.get('source', 'Unknown')\n",
        "                display(Markdown(f\"_Source: {src}_\\n> {node.node.text[:200]}...\"))\n",
        "    question_box.value = ''\n",
        "\n",
        "question_box.on_submit(on_submit)\n",
        "display(question_box, output_box)"
      ],
      "metadata": {
        "id": "UpgBMzCGWG5s",
        "outputId": "02583259-82f8-458b-e7eb-8b227fe067bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b649b85ffdc846ecbca1089dd6cccfe1",
            "8b1ae3e2796b4dafb0000155682120d3",
            "56f5924621df48c086317bc4679f967f",
            "0f3fa9d262b641cdbc34d00c6dc76bf3",
            "5fdfa157754d4cb78ba11b3b2aa6a3d0"
          ]
        }
      },
      "id": "UpgBMzCGWG5s",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Question:', placeholder='Type your question about the insurance policy...')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b649b85ffdc846ecbca1089dd6cccfe1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f3fa9d262b641cdbc34d00c6dc76bf3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Application v3: Advanced Retrieval & Enhanced Query Engine\n",
        "\n",
        "This version introduces:\n",
        "- **Hybrid Search**: Combines semantic similarity with keyword matching (BM25)\n",
        "- **Query Routing**: Different strategies for different question types\n",
        "- **Multi-step Reasoning**: Sub-question generation for complex queries\n",
        "- **Confidence Scoring**: Answer reliability assessment"
      ],
      "metadata": {
        "id": "plV_Cq-35swE"
      },
      "id": "plV_Cq-35swE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install additional packages for v3 features\n",
        "!pip install rank-bm25 sentence-transformers llama-index-question-gen-openai --quiet"
      ],
      "metadata": {
        "id": "Ze_ul8Z75sQC",
        "outputId": "d0761e16-c02b-42a6-b7fa-6bcd5523cbb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ze_ul8Z75sQC",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-index-indices-managed-llama-cloud 0.9.2 requires llama-index-core<0.14,>=0.13.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
            "llama-index-cli 0.5.0 requires llama-index-core<0.14,>=0.13.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
            "llama-index-cli 0.5.0 requires llama-index-llms-openai<0.6,>=0.5.0, but you have llama-index-llms-openai 0.4.7 which is incompatible.\n",
            "llama-index-embeddings-openai 0.5.0 requires llama-index-core<0.14,>=0.13.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
            "llama-index 0.13.3 requires llama-index-core<0.14,>=0.13.3, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
            "llama-index 0.13.3 requires llama-index-llms-openai<0.6,>=0.5.0, but you have llama-index-llms-openai 0.4.7 which is incompatible.\n",
            "llama-index-readers-llama-parse 0.5.0 requires llama-index-core<0.14,>=0.13.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
            "llama-index-readers-file 0.5.2 requires llama-index-core<0.14,>=0.13.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Hybrid Retriever (Semantic + Keyword)\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "\n",
        "# Create semantic retriever\n",
        "vector_retriever = VectorIndexRetriever(index=index_v2, similarity_top_k=5)\n",
        "\n",
        "# Create custom BM25 retriever using rank_bm25\n",
        "class CustomBM25Retriever:\n",
        "    def __init__(self, nodes, similarity_top_k=5):\n",
        "        self.nodes = nodes\n",
        "        self.similarity_top_k = similarity_top_k\n",
        "        # Tokenize documents for BM25\n",
        "        tokenized_docs = [node.text.lower().split() for node in nodes]\n",
        "        self.bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "    def retrieve(self, query_str):\n",
        "        # Ensure we have a string input\n",
        "        if hasattr(query_str, 'query_str'):\n",
        "            query_text = query_str.query_str\n",
        "        elif hasattr(query_str, 'text'):\n",
        "            query_text = query_str.text\n",
        "        else:\n",
        "            query_text = str(query_str)\n",
        "\n",
        "        # Tokenize query\n",
        "        tokenized_query = query_text.lower().split()\n",
        "        # Get BM25 scores\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "        # Get top k indices\n",
        "        top_indices = np.argsort(scores)[::-1][:self.similarity_top_k]\n",
        "        # Return nodes with scores\n",
        "        return [NodeWithScore(node=self.nodes[i], score=scores[i]) for i in top_indices if scores[i] > 0]\n",
        "\n",
        "    # Add async version for compatibility\n",
        "    async def aretrieve(self, query_str):\n",
        "        return self.retrieve(query_str)\n",
        "\n",
        "# Create BM25 retriever\n",
        "bm25_retriever = CustomBM25Retriever(nodes, similarity_top_k=5)\n",
        "\n",
        "# Simple hybrid retriever that combines results\n",
        "class SimpleHybridRetriever:\n",
        "    def __init__(self, vector_retriever, bm25_retriever, similarity_top_k=5):\n",
        "        self.vector_retriever = vector_retriever\n",
        "        self.bm25_retriever = bm25_retriever\n",
        "        self.similarity_top_k = similarity_top_k\n",
        "\n",
        "    def retrieve(self, query_str):\n",
        "        # Ensure we have a string input\n",
        "        if hasattr(query_str, 'query_str'):\n",
        "            query_text = query_str.query_str\n",
        "        elif hasattr(query_str, 'text'):\n",
        "            query_text = query_str.text\n",
        "        else:\n",
        "            query_text = str(query_str)\n",
        "\n",
        "        # Get results from both retrievers\n",
        "        vector_results = self.vector_retriever.retrieve(query_text)\n",
        "        bm25_results = self.bm25_retriever.retrieve(query_text)\n",
        "\n",
        "        # Combine and deduplicate results\n",
        "        all_results = vector_results + bm25_results\n",
        "        seen_texts = set()\n",
        "        unique_results = []\n",
        "\n",
        "        for result in all_results:\n",
        "            if result.node.text not in seen_texts:\n",
        "                seen_texts.add(result.node.text)\n",
        "                unique_results.append(result)\n",
        "\n",
        "        # Return top k results\n",
        "        return unique_results[:self.similarity_top_k]\n",
        "\n",
        "    # Add async version to handle both sync and async calls\n",
        "    async def aretrieve(self, query_str):\n",
        "        return self.retrieve(query_str)\n",
        "\n",
        "hybrid_retriever = SimpleHybridRetriever(vector_retriever, bm25_retriever, similarity_top_k=5)\n",
        "\n",
        "print(\"‚úÖ Hybrid retriever created with async support!\")"
      ],
      "metadata": {
        "id": "99lcwYNK5wsL",
        "outputId": "cb7a206e-e46c-4fed-db85-545cbb4ca63a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "99lcwYNK5wsL",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Hybrid retriever created with async support!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query Routing and Classification\n",
        "import re\n",
        "\n",
        "def classify_question(question):\n",
        "    \"\"\"\n",
        "    Classify question type to route to appropriate strategy\n",
        "    \"\"\"\n",
        "    # Handle both string and QueryBundle objects\n",
        "    if hasattr(question, 'query_str'):\n",
        "        question_text = question.query_str\n",
        "    elif hasattr(question, 'text'):\n",
        "        question_text = question.text\n",
        "    else:\n",
        "        question_text = str(question)\n",
        "\n",
        "    question_lower = question_text.lower()\n",
        "\n",
        "    # Factual questions\n",
        "    if any(word in question_lower for word in ['what', 'who', 'when', 'where', 'which']):\n",
        "        return 'factual'\n",
        "\n",
        "    # Comparison questions\n",
        "    elif any(word in question_lower for word in ['compare', 'difference', 'vs', 'versus', 'better']):\n",
        "        return 'comparison'\n",
        "\n",
        "    # How-to/procedural questions\n",
        "    elif any(word in question_lower for word in ['how', 'process', 'procedure', 'steps']):\n",
        "        return 'procedural'\n",
        "\n",
        "    # Summary questions\n",
        "    elif any(word in question_lower for word in ['summarize', 'summary', 'overview', 'explain']):\n",
        "        return 'summary'\n",
        "\n",
        "    # Default to factual\n",
        "    else:\n",
        "        return 'factual'\n",
        "\n",
        "print(\"Query classification system ready!\")"
      ],
      "metadata": {
        "id": "XXatA6Be5zAc",
        "outputId": "3b3f3e4b-32ba-4e6d-f305-06473a972b82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XXatA6Be5zAc",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classification system ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Query Engines with Multi-step Reasoning\n",
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
        "\n",
        "# Create different query engines for different question types\n",
        "\n",
        "# 1. Standard hybrid query engine\n",
        "hybrid_query_engine = RetrieverQueryEngine(\n",
        "    retriever=hybrid_retriever,\n",
        "    response_synthesizer=get_response_synthesizer(response_mode=\"compact\")\n",
        ")\n",
        "\n",
        "# 2. Try to create sub-question query engine for complex queries\n",
        "try:\n",
        "    query_engine_tools = [\n",
        "        QueryEngineTool(\n",
        "            query_engine=hybrid_query_engine,\n",
        "            metadata=ToolMetadata(\n",
        "                name=\"insurance_policy\",\n",
        "                description=\"Provides information about insurance policy details, coverage, terms, and conditions\"\n",
        "            )\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
        "        query_engine_tools=query_engine_tools,\n",
        "        llm=llm\n",
        "    )\n",
        "    print(\"Enhanced query engines created successfully!\")\n",
        "\n",
        "except (ImportError, AttributeError) as e:\n",
        "    print(f\"SubQuestionQueryEngine not available: {e}\")\n",
        "    print(\"Using standard hybrid query engine for all queries.\")\n",
        "    # Fallback: use hybrid query engine for all question types\n",
        "    sub_question_engine = hybrid_query_engine"
      ],
      "metadata": {
        "id": "Zwb1is_c52Yd",
        "outputId": "18ee334e-22a3-4e3e-9392-619cfc549946",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Zwb1is_c52Yd",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced query engines created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confidence Scoring System\n",
        "def calculate_confidence_score(response, retrieved_nodes):\n",
        "    \"\"\"\n",
        "    Calculate confidence score based on multiple factors\n",
        "    \"\"\"\n",
        "    score = 0.0\n",
        "    factors = []\n",
        "    response_text = response.lower()\n",
        "\n",
        "    # Factor 1: Number of supporting sources (max 25 points)\n",
        "    num_sources = len(retrieved_nodes) if retrieved_nodes else 0\n",
        "    source_score = min(num_sources * 5, 25)  # Up to 5 sources\n",
        "    score += source_score\n",
        "    factors.append(f\"Sources: {num_sources} (+{source_score}pts)\")\n",
        "\n",
        "    # Factor 2: Response length and completeness (max 20 points)\n",
        "    response_length = len(response.split())\n",
        "    if 30 <= response_length <= 150:\n",
        "        length_score = 20  # Optimal length\n",
        "    elif 20 <= response_length < 30 or 150 < response_length <= 200:\n",
        "        length_score = 15  # Good length\n",
        "    elif 10 <= response_length < 20 or 200 < response_length <= 300:\n",
        "        length_score = 10  # Acceptable length\n",
        "    else:\n",
        "        length_score = 5   # Too short or too long\n",
        "    score += length_score\n",
        "    factors.append(f\"Length: {response_length} words (+{length_score}pts)\")\n",
        "\n",
        "    # Factor 3: Specific policy references (max 25 points)\n",
        "    specific_indicators = [\n",
        "        'section', 'page', 'part', 'according to', 'states that', 'specifically',\n",
        "        'outlined', 'policy', 'coverage', 'benefit', 'procedure', 'days', 'within'\n",
        "    ]\n",
        "    specificity_count = sum(1 for word in specific_indicators if word in response_text)\n",
        "    specificity_score = min(specificity_count * 3, 25)\n",
        "    score += specificity_score\n",
        "    factors.append(f\"Policy specificity: {specificity_count} terms (+{specificity_score}pts)\")\n",
        "\n",
        "    # Factor 4: Uncertainty and generic responses (penalty)\n",
        "    uncertainty_phrases = [\n",
        "        'not sure', 'unclear', 'might be', 'possibly', 'perhaps', 'generally',\n",
        "        'typically', 'usually', 'contact the', 'consult with', 'it is advisable'\n",
        "    ]\n",
        "    uncertainty_count = sum(1 for phrase in uncertainty_phrases if phrase in response_text)\n",
        "    uncertainty_penalty = min(uncertainty_count * 8, 20)  # Max 20 point penalty\n",
        "    score -= uncertainty_penalty\n",
        "    if uncertainty_penalty > 0:\n",
        "        factors.append(f\"Generic/uncertain language: -{uncertainty_penalty}pts\")\n",
        "\n",
        "    # Factor 5: Numerical precision bonus (max 15 points)\n",
        "    numbers_found = len([word for word in response.split() if any(char.isdigit() for char in word)])\n",
        "    precision_score = min(numbers_found * 3, 15)  # Numbers suggest specific data\n",
        "    score += precision_score\n",
        "    if precision_score > 0:\n",
        "        factors.append(f\"Numerical precision: {numbers_found} values (+{precision_score}pts)\")\n",
        "\n",
        "    # Factor 6: Source quality assessment (max 15 points)\n",
        "    if retrieved_nodes:\n",
        "        # Check if sources contain substantial content (not just headers)\n",
        "        substantial_sources = 0\n",
        "        for node in retrieved_nodes:\n",
        "            if len(node.node.text.strip()) > 100:  # More than just headers\n",
        "                substantial_sources += 1\n",
        "\n",
        "        source_quality = min(substantial_sources * 5, 15)\n",
        "        score += source_quality\n",
        "        if source_quality > 0:\n",
        "            factors.append(f\"Source quality: {substantial_sources} substantial (+{source_quality}pts)\")\n",
        "\n",
        "    # Normalize to 0-100 scale and add some variability\n",
        "    import random\n",
        "    variability = random.uniform(-3, 3)  # Small random factor to avoid identical scores\n",
        "    final_score = max(0, min(100, score + variability))\n",
        "\n",
        "    return round(final_score), factors\n",
        "\n",
        "print(\"Enhanced confidence scoring system ready!\")"
      ],
      "metadata": {
        "id": "yf2-3F_Y54DT",
        "outputId": "a3d83bb8-c0d7-498a-b7ad-9523f9f1ea59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yf2-3F_Y54DT",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced confidence scoring system ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Chat Interface with Persistent History\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output, HTML\n",
        "import time\n",
        "\n",
        "# Reset chat history for new session\n",
        "chat_history_v3_enhanced = []\n",
        "\n",
        "# Create UI components\n",
        "question_box_enhanced = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Ask about your insurance policy (Enhanced v3 with persistent history)...',\n",
        "    description='Question:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='700px')\n",
        ")\n",
        "\n",
        "# Create a scrollable output area\n",
        "output_area_enhanced = widgets.Output(\n",
        "    layout=widgets.Layout(\n",
        "        height='400px',\n",
        "        width='100%',\n",
        "        border='1px solid #ccc',\n",
        "        overflow_y='auto'\n",
        "    )\n",
        ")\n",
        "\n",
        "def display_chat_history():\n",
        "    \"\"\"Display the entire chat history in a formatted way\"\"\"\n",
        "    with output_area_enhanced:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        if not chat_history_v3_enhanced:\n",
        "            display(Markdown(\"*Start your conversation by asking a question about your insurance policy...*\"))\n",
        "            return\n",
        "\n",
        "        for i in range(0, len(chat_history_v3_enhanced), 2):\n",
        "            if i + 1 < len(chat_history_v3_enhanced):\n",
        "                user_msg = chat_history_v3_enhanced[i]\n",
        "                assistant_msg = chat_history_v3_enhanced[i + 1]\n",
        "\n",
        "                # Display exchange number\n",
        "                exchange_num = (i // 2) + 1\n",
        "                display(Markdown(f\"### üí¨ Exchange {exchange_num}\"))\n",
        "\n",
        "                # Display question\n",
        "                display(Markdown(f\"**ü§î Q:** {user_msg['content']}\"))\n",
        "\n",
        "                # Display answer with metadata if available\n",
        "                response_content = assistant_msg['content']\n",
        "                if isinstance(assistant_msg.get('metadata'), dict):\n",
        "                    meta = assistant_msg['metadata']\n",
        "                    context_indicator = \"üîÑ\" if meta.get('context_used', False) else \"üÜï\"\n",
        "                    display(Markdown(f\"**üìä Analysis:** {context_indicator} Type: `{meta.get('question_type', 'unknown')}` | Time: `{meta.get('processing_time', 0):.2f}s` | Confidence: {meta.get('confidence', 0):.0f}/100\"))\n",
        "\n",
        "                    # Show sub-question information if available (formatted)\n",
        "                    if meta.get('sub_questions_info'):\n",
        "                        # Parse and format sub-question information\n",
        "                        sub_info = meta['sub_questions_info']\n",
        "                        if 'Generated' in sub_info and 'sub questions' in sub_info:\n",
        "                            # Extract number of sub-questions\n",
        "                            import re\n",
        "                            match = re.search(r'Generated (\\d+) sub questions', sub_info)\n",
        "                            if match:\n",
        "                                num_questions = match.group(1)\n",
        "                                display(Markdown(f\"**üîç Query Processing:** Used multi-step reasoning with {num_questions} sub-questions\"))\n",
        "                        else:\n",
        "                            display(Markdown(f\"**üîç Query Processing:** {sub_info}\"))\n",
        "\n",
        "                display(Markdown(f\"**ü§ñ A:** {response_content}\"))\n",
        "\n",
        "                # Enhanced source citation with page numbers and sections\n",
        "                if isinstance(assistant_msg.get('metadata'), dict) and assistant_msg['metadata'].get('source_nodes'):\n",
        "                    source_nodes = assistant_msg['metadata']['source_nodes']\n",
        "                    if source_nodes:\n",
        "                        display(Markdown(\"**üìö Sources Referenced:**\"))\n",
        "                        for i, node in enumerate(source_nodes[:3], 1):  # Show top 3 sources\n",
        "                            # Extract source information\n",
        "                            source_meta = node.node.metadata\n",
        "                            page_info = source_meta.get('page_label', source_meta.get('source', 'Unknown'))\n",
        "\n",
        "                            # Get text preview\n",
        "                            text_preview = node.node.text[:120].replace('\\n', ' ').strip()\n",
        "\n",
        "                            # Format source citation\n",
        "                            if page_info != 'Unknown':\n",
        "                                display(Markdown(f\"**{i}.** Page {page_info}: *\\\"{text_preview}...\\\"*\"))\n",
        "                            else:\n",
        "                                display(Markdown(f\"**{i}.** Document Section: *\\\"{text_preview}...\\\"*\"))\n",
        "\n",
        "                display(Markdown(\"---\"))\n",
        "\n",
        "def enhanced_query_processing(question):\n",
        "    \"\"\"Enhanced query processing with better context handling\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Ensure we work with string input\n",
        "    question_str = str(question).strip()\n",
        "\n",
        "    # Step 1: Classify question type\n",
        "    question_type = classify_question(question_str)\n",
        "\n",
        "    # Step 2: Enhanced context handling using the enhanced history\n",
        "    if chat_history_v3_enhanced:\n",
        "        # Get last 2 exchanges for context\n",
        "        recent_history = chat_history_v3_enhanced[-4:]\n",
        "\n",
        "        # Detect follow-up questions\n",
        "        follow_up_indicators = [\n",
        "            'elaborate', 'explain more', 'tell me more', 'expand', 'details',\n",
        "            'that', 'it', 'this', 'further', 'more about', 'specific',\n",
        "            'can you', 'what about', 'how about'\n",
        "        ]\n",
        "        is_follow_up = any(indicator in question_str.lower() for indicator in follow_up_indicators)\n",
        "\n",
        "        if is_follow_up and len(recent_history) >= 2:\n",
        "            # Enhanced follow-up handling\n",
        "            last_question = recent_history[-2]['content'] if recent_history[-2]['role'] == 'user' else \"\"\n",
        "            last_answer = recent_history[-1]['content'] if recent_history[-1]['role'] == 'assistant' else \"\"\n",
        "\n",
        "            contextual_question = f\"\"\"Previous Question: {last_question}\n",
        "Previous Answer: {last_answer}\n",
        "\n",
        "User Follow-up Request: {question_str}\n",
        "\n",
        "Please provide more detailed information, elaborate further, or answer the follow-up question about the same topic.\"\"\"\n",
        "        else:\n",
        "            # Regular context for independent questions\n",
        "            context_str = \"\\n\".join([\n",
        "                f\"{msg['role'].title()}: {msg['content'][:100]}...\" if len(msg['content']) > 100 else f\"{msg['role'].title()}: {msg['content']}\"\n",
        "                for msg in recent_history\n",
        "            ])\n",
        "            contextual_question = f\"Context:\\n{context_str}\\n\\nNew Question: {question_str}\"\n",
        "    else:\n",
        "        contextual_question = question_str\n",
        "\n",
        "    # Step 3: Route to appropriate query engine with output capture\n",
        "    import sys\n",
        "    from io import StringIO\n",
        "    import contextlib\n",
        "\n",
        "    # Capture sub-question engine output\n",
        "    captured_output = StringIO()\n",
        "\n",
        "    with contextlib.redirect_stdout(captured_output):\n",
        "        if question_type in ['comparison', 'summary'] or len(question_str.split()) > 15:\n",
        "            response = sub_question_engine.query(contextual_question)\n",
        "        else:\n",
        "            response = hybrid_query_engine.query(contextual_question)\n",
        "\n",
        "    # Get and clean captured sub-question information\n",
        "    sub_questions_output = captured_output.getvalue()\n",
        "\n",
        "    # Clean and format the sub-question output\n",
        "    cleaned_sub_info = None\n",
        "    if sub_questions_output.strip():\n",
        "        # Remove extra whitespace and format\n",
        "        lines = [line.strip() for line in sub_questions_output.strip().split('\\n') if line.strip()]\n",
        "        if lines:\n",
        "            # Join meaningful lines\n",
        "            cleaned_sub_info = ' | '.join(lines[:3])  # Take first 3 meaningful lines\n",
        "\n",
        "    # Step 4: Calculate confidence\n",
        "    source_nodes = getattr(response, 'source_nodes', [])\n",
        "    confidence, factors = calculate_confidence_score(response.response, source_nodes)\n",
        "\n",
        "    processing_time = time.time() - start_time\n",
        "\n",
        "    return {\n",
        "        'response': response,\n",
        "        'question_type': question_type,\n",
        "        'confidence': confidence,\n",
        "        'factors': factors,\n",
        "        'processing_time': processing_time,\n",
        "        'source_nodes': source_nodes,\n",
        "        'context_used': len(chat_history_v3_enhanced) > 0,\n",
        "        'sub_questions_info': cleaned_sub_info\n",
        "    }\n",
        "\n",
        "def on_submit_enhanced(sender):\n",
        "    question = question_box_enhanced.value.strip()\n",
        "    if not question:\n",
        "        return\n",
        "\n",
        "    if question.lower() == 'exit':\n",
        "        question_box_enhanced.disabled = True\n",
        "        with output_area_enhanced:\n",
        "            clear_output()\n",
        "            display(Markdown(\"**üîö Chat session ended. Run the cell again to restart.**\"))\n",
        "        return\n",
        "\n",
        "    if question.lower() == 'clear':\n",
        "        # Clear all conversation histories\n",
        "        chat_history_v3_enhanced.clear()\n",
        "        # Also clear the regular v3 history used by other components\n",
        "        global chat_history_v3\n",
        "        chat_history_v3.clear()\n",
        "\n",
        "        # Clear ALL outputs including sub-question engine outputs\n",
        "        from IPython.display import clear_output as global_clear_output\n",
        "        global_clear_output(wait=True)\n",
        "\n",
        "        # Re-display the interface\n",
        "        display(Markdown(\"### üöÄ Enhanced RAG Chat (v3+)\\n*Features: Persistent History, Better Follow-ups, Scrollable Output*\"))\n",
        "        display(question_box_enhanced)\n",
        "        display(output_area_enhanced)\n",
        "\n",
        "        # Reset the display with cleared message\n",
        "        display_chat_history()\n",
        "        question_box_enhanced.value = ''\n",
        "\n",
        "        # Show confirmation message\n",
        "        with output_area_enhanced:\n",
        "            display(Markdown(\"‚úÖ **Conversation history cleared!** All context has been reset.\"))\n",
        "        return\n",
        "\n",
        "    # Add user question to history\n",
        "    chat_history_v3_enhanced.append({'role': 'user', 'content': question})\n",
        "\n",
        "    # Show processing message\n",
        "    with output_area_enhanced:\n",
        "        # Keep existing history and add processing message\n",
        "        display(Markdown(f\"**ü§î Q:** {question}\"))\n",
        "        display(Markdown(\"*üîÑ Processing with enhanced v3 features...*\"))\n",
        "\n",
        "    try:\n",
        "        # Process the question\n",
        "        result = enhanced_query_processing(question)\n",
        "\n",
        "        # Add assistant response with metadata to history\n",
        "        chat_history_v3_enhanced.append({\n",
        "            'role': 'assistant',\n",
        "            'content': result['response'].response,\n",
        "            'metadata': {\n",
        "                'question_type': result['question_type'],\n",
        "                'confidence': result['confidence'],\n",
        "                'processing_time': result['processing_time'],\n",
        "                'context_used': result['context_used'],\n",
        "                'sub_questions_info': result.get('sub_questions_info'),\n",
        "                'source_nodes': result.get('source_nodes', [])\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # Refresh the display with complete history\n",
        "        display_chat_history()\n",
        "\n",
        "    except Exception as e:\n",
        "        with output_area_enhanced:\n",
        "            display(Markdown(f\"**‚ùå Error:** {str(e)}\"))\n",
        "\n",
        "    question_box_enhanced.value = ''\n",
        "\n",
        "# Set up the interface\n",
        "question_box_enhanced.on_submit(on_submit_enhanced)\n",
        "\n",
        "# Display the enhanced interface\n",
        "display(Markdown(\"### üöÄ Enhanced RAG Chat (v3+)\\n*Features: Persistent History, Better Follow-ups, Scrollable Output*\"))\n",
        "display(question_box_enhanced)\n",
        "display(output_area_enhanced)\n",
        "\n",
        "# Initialize with welcome message\n",
        "display_chat_history()"
      ],
      "metadata": {
        "id": "mB7V15rmGLkU",
        "outputId": "15bf94bb-40f9-4b32-ba06-8868621ebc7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "0c2c7c137fc34632baa10ccbb6d10a82",
            "35ce380b32504b14a5a55b134ad97b9a",
            "e074529b34f34b2cba39df2fd3a372c5",
            "a65482c7be8c4c2abf19e48041aaf43a",
            "38caca798ccc479e8f77049b8a4f372c"
          ]
        }
      },
      "id": "mB7V15rmGLkU",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### üöÄ Enhanced RAG Chat (v3+)\n*Features: Persistent History, Better Follow-ups, Scrollable Output*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='clear', description='Question:', layout=Layout(width='700px'), placeholder='Ask about your insuran‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c2c7c137fc34632baa10ccbb6d10a82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output(layout=Layout(border='1px solid #ccc', height='400px', overflow_y='auto', width='100%'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a65482c7be8c4c2abf19e48041aaf43a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Hybrid Retriever (Semantic + Keyword)\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "\n",
        "# Create semantic retriever\n",
        "vector_retriever = VectorIndexRetriever(index=index_v2, similarity_top_k=5)\n",
        "\n",
        "# Create custom BM25 retriever with content quality boosting\n",
        "class CustomBM25Retriever:\n",
        "    def __init__(self, nodes, similarity_top_k=5):\n",
        "        self.nodes = nodes\n",
        "        self.similarity_top_k = similarity_top_k\n",
        "        # Tokenize documents for BM25\n",
        "        tokenized_docs = [node.text.lower().split() for node in nodes]\n",
        "        self.bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "    def _boost_content_quality(self, scores, query_text):\n",
        "        \"\"\"\n",
        "        Boost scores for content-rich nodes and penalize structural content\n",
        "        \"\"\"\n",
        "        boosted_scores = scores.copy()\n",
        "        query_lower = query_text.lower()\n",
        "\n",
        "        for i, node in enumerate(self.nodes):\n",
        "            node_text = node.text.lower()\n",
        "\n",
        "            # Heavy penalties for table of contents and structural content\n",
        "            severe_penalty_phrases = [\n",
        "                'table of contents', 'gc 6001 table of contents',\n",
        "                'this policy has been updated effective january 1, 2014 gc 6001'\n",
        "            ]\n",
        "\n",
        "            moderate_penalty_phrases = [\n",
        "                'section a -', 'section b -', 'section c -', 'section d -',\n",
        "                'part i -', 'part ii -', 'part iii -', 'part iv -',\n",
        "                'page 1', 'page 2', 'page 3', 'page 4', 'page 5'\n",
        "            ]\n",
        "\n",
        "            # Apply severe penalties\n",
        "            for phrase in severe_penalty_phrases:\n",
        "                if phrase in node_text:\n",
        "                    boosted_scores[i] *= 0.01  # Nearly eliminate table of contents\n",
        "                    break\n",
        "            else:\n",
        "                # Apply moderate penalties if no severe penalty applied\n",
        "                for phrase in moderate_penalty_phrases:\n",
        "                    if phrase in node_text and len(node_text) < 300:\n",
        "                        boosted_scores[i] *= 0.3  # Reduce structural content\n",
        "                        break\n",
        "\n",
        "            # Boost content-rich sections\n",
        "            if any(term in query_lower for term in ['exclusion', 'procedure', 'payment', 'claim']):\n",
        "                content_boost_phrases = [\n",
        "                    'coverage exclusion', 'claim procedure', 'premium payment',\n",
        "                    'death benefit', 'proof of loss', 'notice of claim',\n",
        "                    'medical examination', 'autopsy', 'legal action'\n",
        "                ]\n",
        "\n",
        "                for phrase in content_boost_phrases:\n",
        "                    if phrase in node_text:\n",
        "                        boosted_scores[i] *= 1.5  # Boost relevant content\n",
        "                        break\n",
        "\n",
        "        return boosted_scores\n",
        "\n",
        "    def retrieve(self, query_str):\n",
        "        # Ensure we have a string input\n",
        "        if hasattr(query_str, 'query_str'):\n",
        "            query_text = query_str.query_str\n",
        "        elif hasattr(query_str, 'text'):\n",
        "            query_text = query_str.text\n",
        "        else:\n",
        "            query_text = str(query_str)\n",
        "\n",
        "        # Tokenize query\n",
        "        tokenized_query = query_text.lower().split()\n",
        "        # Get BM25 scores\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "\n",
        "        # Apply content quality boosting\n",
        "        boosted_scores = self._boost_content_quality(scores, query_text)\n",
        "\n",
        "        # Get top k indices\n",
        "        top_indices = np.argsort(boosted_scores)[::-1][:self.similarity_top_k]\n",
        "        # Return nodes with scores\n",
        "        return [NodeWithScore(node=self.nodes[i], score=boosted_scores[i]) for i in top_indices if boosted_scores[i] > 0]\n",
        "\n",
        "    # Add async version for compatibility\n",
        "    async def aretrieve(self, query_str):\n",
        "        return self.retrieve(query_str)\n",
        "\n",
        "# Create BM25 retriever\n",
        "bm25_retriever = CustomBM25Retriever(nodes, similarity_top_k=5)\n",
        "\n",
        "# Simple hybrid retriever that combines results with content filtering\n",
        "class SimpleHybridRetriever:\n",
        "    def __init__(self, vector_retriever, bm25_retriever, similarity_top_k=5):\n",
        "        self.vector_retriever = vector_retriever\n",
        "        self.bm25_retriever = bm25_retriever\n",
        "        self.similarity_top_k = similarity_top_k\n",
        "\n",
        "    def _is_substantial_content(self, node):\n",
        "        \"\"\"\n",
        "        Filter out low-quality content like table of contents, headers, etc.\n",
        "        \"\"\"\n",
        "        text = node.text.lower().strip()\n",
        "\n",
        "        # Strict filter for table of contents and structural content\n",
        "        strict_filter_phrases = [\n",
        "            'table of contents',\n",
        "            'gc 6001 table of contents',\n",
        "            'this policy has been updated effective january 1, 2014 gc 6001'\n",
        "        ]\n",
        "\n",
        "        # Hard reject these regardless of length\n",
        "        for phrase in strict_filter_phrases:\n",
        "            if phrase in text:\n",
        "                return False\n",
        "\n",
        "        # Filter out very short structural content\n",
        "        if len(text.strip()) < 100:\n",
        "            return False\n",
        "\n",
        "        # Less aggressive filtering for medium-length content\n",
        "        if len(text) < 200:\n",
        "            structural_phrases = [\n",
        "                'section a -', 'section b -', 'section c -', 'section d -',\n",
        "                'part i -', 'part ii -', 'part iii -', 'part iv -'\n",
        "            ]\n",
        "            for phrase in structural_phrases:\n",
        "                if phrase in text:\n",
        "                    return False\n",
        "\n",
        "        # Check for actual content indicators (more lenient)\n",
        "        content_indicators = [\n",
        "            'coverage', 'benefit', 'exclusion', 'procedure', 'payment',\n",
        "            'claim', 'premium', 'death', 'accident', 'medical',\n",
        "            'within', 'days', 'shall', 'must', 'required', 'employee',\n",
        "            'insurance', 'policy', 'amount', 'termination', 'effective'\n",
        "        ]\n",
        "\n",
        "        # Lower threshold for content indicators\n",
        "        content_score = sum(1 for indicator in content_indicators if indicator in text)\n",
        "        return content_score >= 1  # Require at least 1 content indicator (less strict)\n",
        "\n",
        "    def retrieve(self, query_str):\n",
        "        # Ensure we have a string input\n",
        "        if hasattr(query_str, 'query_str'):\n",
        "            query_text = query_str.query_str\n",
        "        elif hasattr(query_str, 'text'):\n",
        "            query_text = query_str.text\n",
        "        else:\n",
        "            query_text = str(query_str)\n",
        "\n",
        "        # Get results from both retrievers\n",
        "        vector_results = self.vector_retriever.retrieve(query_text)\n",
        "        bm25_results = self.bm25_retriever.retrieve(query_text)\n",
        "\n",
        "        # Combine and filter for substantial content\n",
        "        all_results = vector_results + bm25_results\n",
        "        seen_texts = set()\n",
        "        filtered_results = []\n",
        "\n",
        "        for result in all_results:\n",
        "            # Skip if already seen\n",
        "            if result.node.text in seen_texts:\n",
        "                continue\n",
        "\n",
        "            # Apply content filtering\n",
        "            if self._is_substantial_content(result.node):\n",
        "                seen_texts.add(result.node.text)\n",
        "                filtered_results.append(result)\n",
        "\n",
        "        # If we have too few substantial results, add selective backup\n",
        "        if len(filtered_results) < 2:\n",
        "            for result in all_results:\n",
        "                if result.node.text not in seen_texts and len(filtered_results) < self.similarity_top_k:\n",
        "                    text = result.node.text.lower().strip()\n",
        "                    # Strict exclusion of table of contents even in backup\n",
        "                    if ('table of contents' in text or\n",
        "                        'gc 6001 table of contents' in text or\n",
        "                        len(text) < 80):\n",
        "                        continue\n",
        "\n",
        "                    # Only include if it has policy-related content\n",
        "                    if any(word in text for word in ['coverage', 'benefit', 'claim', 'insurance', 'policy', 'employee', 'procedure']):\n",
        "                        filtered_results.append(result)\n",
        "                        seen_texts.add(result.node.text)\n",
        "\n",
        "        # Return top k results\n",
        "        return filtered_results[:self.similarity_top_k]\n",
        "\n",
        "    # Add async version to handle both sync and async calls\n",
        "    async def aretrieve(self, query_str):\n",
        "        return self.retrieve(query_str)\n",
        "\n",
        "hybrid_retriever = SimpleHybridRetriever(vector_retriever, bm25_retriever, similarity_top_k=5)\n",
        "\n",
        "print(\"‚úÖ Hybrid retriever created with async support!\")"
      ],
      "metadata": {
        "id": "cjfqFJNEJG-L",
        "outputId": "976cc399-9e91-4c31-a367-72543e181e3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cjfqFJNEJG-L",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Hybrid retriever created with async support!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query Routing and Classification\n",
        "import re\n",
        "\n",
        "def classify_question(question):\n",
        "    \"\"\"\n",
        "    Classify question type to route to appropriate strategy\n",
        "    \"\"\"\n",
        "    # Handle both string and QueryBundle objects\n",
        "    if hasattr(question, 'query_str'):\n",
        "        question_text = question.query_str\n",
        "    elif hasattr(question, 'text'):\n",
        "        question_text = question.text\n",
        "    else:\n",
        "        question_text = str(question)\n",
        "\n",
        "    question_lower = question_text.lower()\n",
        "\n",
        "    # Factual questions\n",
        "    if any(word in question_lower for word in ['what', 'who', 'when', 'where', 'which']):\n",
        "        return 'factual'\n",
        "\n",
        "    # Comparison questions\n",
        "    elif any(word in question_lower for word in ['compare', 'difference', 'vs', 'versus', 'better']):\n",
        "        return 'comparison'\n",
        "\n",
        "    # How-to/procedural questions\n",
        "    elif any(word in question_lower for word in ['how', 'process', 'procedure', 'steps']):\n",
        "        return 'procedural'\n",
        "\n",
        "    # Summary questions\n",
        "    elif any(word in question_lower for word in ['summarize', 'summary', 'overview', 'explain']):\n",
        "        return 'summary'\n",
        "\n",
        "    # Default to factual\n",
        "    else:\n",
        "        return 'factual'\n",
        "\n",
        "print(\"Query classification system ready!\")"
      ],
      "metadata": {
        "id": "P4PtnHgWRebN",
        "outputId": "b1c98d29-b4e6-45c1-fc70-b68e17953060",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "P4PtnHgWRebN",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classification system ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Query Engines with Multi-step Reasoning\n",
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
        "\n",
        "# Create different query engines for different question types\n",
        "\n",
        "# 1. Standard hybrid query engine\n",
        "hybrid_query_engine = RetrieverQueryEngine(\n",
        "    retriever=hybrid_retriever,\n",
        "    response_synthesizer=get_response_synthesizer(response_mode=\"compact\")\n",
        ")\n",
        "\n",
        "# 2. Try to create sub-question query engine for complex queries\n",
        "try:\n",
        "    query_engine_tools = [\n",
        "        QueryEngineTool(\n",
        "            query_engine=hybrid_query_engine,\n",
        "            metadata=ToolMetadata(\n",
        "                name=\"insurance_policy\",\n",
        "                description=\"Provides information about insurance policy details, coverage, terms, and conditions\"\n",
        "            )\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
        "        query_engine_tools=query_engine_tools,\n",
        "        llm=llm\n",
        "    )\n",
        "    print(\"Enhanced query engines created successfully!\")\n",
        "\n",
        "except (ImportError, AttributeError) as e:\n",
        "    print(f\"SubQuestionQueryEngine not available: {e}\")\n",
        "    print(\"Using standard hybrid query engine for all queries.\")\n",
        "    # Fallback: use hybrid query engine for all question types\n",
        "    sub_question_engine = hybrid_query_engine"
      ],
      "metadata": {
        "id": "czn5D5HiRiuq",
        "outputId": "6ba35a47-57bc-4322-f3b7-f06980a577bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "czn5D5HiRiuq",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced query engines created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confidence Scoring System\n",
        "def calculate_confidence_score(response, retrieved_nodes):\n",
        "    \"\"\"\n",
        "    Calculate confidence score based on multiple factors\n",
        "    \"\"\"\n",
        "    score = 0.0\n",
        "    factors = []\n",
        "    response_text = response.lower()\n",
        "\n",
        "    # Factor 1: Number of supporting sources (max 25 points)\n",
        "    num_sources = len(retrieved_nodes) if retrieved_nodes else 0\n",
        "    source_score = min(num_sources * 5, 25)  # Up to 5 sources\n",
        "    score += source_score\n",
        "    factors.append(f\"Sources: {num_sources} (+{source_score}pts)\")\n",
        "\n",
        "    # Factor 2: Response length and completeness (max 20 points)\n",
        "    response_length = len(response.split())\n",
        "    if 30 <= response_length <= 150:\n",
        "        length_score = 20  # Optimal length\n",
        "    elif 20 <= response_length < 30 or 150 < response_length <= 200:\n",
        "        length_score = 15  # Good length\n",
        "    elif 10 <= response_length < 20 or 200 < response_length <= 300:\n",
        "        length_score = 10  # Acceptable length\n",
        "    else:\n",
        "        length_score = 5   # Too short or too long\n",
        "    score += length_score\n",
        "    factors.append(f\"Length: {response_length} words (+{length_score}pts)\")\n",
        "\n",
        "    # Factor 3: Specific policy references (max 25 points)\n",
        "    specific_indicators = [\n",
        "        'section', 'page', 'part', 'according to', 'states that', 'specifically',\n",
        "        'outlined', 'policy', 'coverage', 'benefit', 'procedure', 'days', 'within'\n",
        "    ]\n",
        "    specificity_count = sum(1 for word in specific_indicators if word in response_text)\n",
        "    specificity_score = min(specificity_count * 3, 25)\n",
        "    score += specificity_score\n",
        "    factors.append(f\"Policy specificity: {specificity_count} terms (+{specificity_score}pts)\")\n",
        "\n",
        "    # Factor 4: Uncertainty and generic responses (penalty)\n",
        "    uncertainty_phrases = [\n",
        "        'not sure', 'unclear', 'might be', 'possibly', 'perhaps', 'generally',\n",
        "        'typically', 'usually', 'contact the', 'consult with', 'it is advisable'\n",
        "    ]\n",
        "    uncertainty_count = sum(1 for phrase in uncertainty_phrases if phrase in response_text)\n",
        "    uncertainty_penalty = min(uncertainty_count * 8, 20)  # Max 20 point penalty\n",
        "    score -= uncertainty_penalty\n",
        "    if uncertainty_penalty > 0:\n",
        "        factors.append(f\"Generic/uncertain language: -{uncertainty_penalty}pts\")\n",
        "\n",
        "    # Factor 5: Numerical precision bonus (max 15 points)\n",
        "    numbers_found = len([word for word in response.split() if any(char.isdigit() for char in word)])\n",
        "    precision_score = min(numbers_found * 3, 15)  # Numbers suggest specific data\n",
        "    score += precision_score\n",
        "    if precision_score > 0:\n",
        "        factors.append(f\"Numerical precision: {numbers_found} values (+{precision_score}pts)\")\n",
        "\n",
        "    # Factor 6: Enhanced source quality assessment (max 20 points)\n",
        "    if retrieved_nodes:\n",
        "        substantial_sources = 0\n",
        "        content_quality_bonus = 0\n",
        "\n",
        "        for node in retrieved_nodes:\n",
        "            node_text = node.node.text.lower().strip()\n",
        "\n",
        "            # Check for substantial content length\n",
        "            if len(node_text) > 150:\n",
        "                substantial_sources += 1\n",
        "\n",
        "                # Additional quality bonuses\n",
        "                # Penalty for table of contents and structural content\n",
        "                if any(phrase in node_text for phrase in [\n",
        "                    'table of contents', 'this policy has been updated effective',\n",
        "                    'section a -', 'part i -'\n",
        "                ]):\n",
        "                    content_quality_bonus -= 2  # Penalty for low-quality sources\n",
        "\n",
        "                # Bonus for content-rich sources\n",
        "                elif any(phrase in node_text for phrase in [\n",
        "                    'coverage amount', 'exclusion', 'claim procedure', 'premium payment',\n",
        "                    'death benefit', 'medical examination', 'proof of loss'\n",
        "                ]):\n",
        "                    content_quality_bonus += 3  # Bonus for relevant content\n",
        "\n",
        "        # Calculate source quality score\n",
        "        base_quality = min(substantial_sources * 4, 16)  # Base score for substantial sources\n",
        "        quality_bonus = max(-8, min(8, content_quality_bonus))  # Bonus/penalty for content quality\n",
        "        source_quality = max(0, base_quality + quality_bonus)\n",
        "\n",
        "        score += source_quality\n",
        "        if source_quality > 0:\n",
        "            factors.append(f\"Source quality: {substantial_sources} substantial (+{source_quality}pts)\")\n",
        "        elif substantial_sources == 0:\n",
        "            factors.append(f\"Source quality: Low-quality sources (-5pts)\")\n",
        "            score -= 5  # Penalty for no substantial sources\n",
        "\n",
        "    # Normalize to 0-100 scale and add some variability\n",
        "    import random\n",
        "    variability = random.uniform(-3, 3)  # Small random factor to avoid identical scores\n",
        "    final_score = max(0, min(100, score + variability))\n",
        "\n",
        "    return round(final_score), factors\n",
        "\n",
        "print(\"Enhanced confidence scoring system ready!\")"
      ],
      "metadata": {
        "id": "3MTKJXKlRmXz",
        "outputId": "aadc6eca-bde6-455b-88d1-8dd96477ad51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3MTKJXKlRmXz",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced confidence scoring system ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Chat Interface with Persistent History\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output, HTML\n",
        "import time\n",
        "\n",
        "# Reset chat history for new session\n",
        "chat_history_v3_enhanced = []\n",
        "\n",
        "# Create UI components\n",
        "question_box_enhanced = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Ask about your insurance policy (Enhanced v3 with persistent history)...',\n",
        "    description='Question:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='700px')\n",
        ")\n",
        "\n",
        "# Create a scrollable output area\n",
        "output_area_enhanced = widgets.Output(\n",
        "    layout=widgets.Layout(\n",
        "        height='400px',\n",
        "        width='100%',\n",
        "        border='1px solid #ccc',\n",
        "        overflow_y='auto'\n",
        "    )\n",
        ")\n",
        "\n",
        "def display_chat_history():\n",
        "    \"\"\"Display the entire chat history in a formatted way\"\"\"\n",
        "    with output_area_enhanced:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        if not chat_history_v3_enhanced:\n",
        "            display(Markdown(\"*Start your conversation by asking a question about your insurance policy...*\"))\n",
        "            return\n",
        "\n",
        "        for i in range(0, len(chat_history_v3_enhanced), 2):\n",
        "            if i + 1 < len(chat_history_v3_enhanced):\n",
        "                user_msg = chat_history_v3_enhanced[i]\n",
        "                assistant_msg = chat_history_v3_enhanced[i + 1]\n",
        "\n",
        "                # Display exchange number\n",
        "                exchange_num = (i // 2) + 1\n",
        "                display(Markdown(f\"### üí¨ Exchange {exchange_num}\"))\n",
        "\n",
        "                # Display question\n",
        "                display(Markdown(f\"**ü§î Q:** {user_msg['content']}\"))\n",
        "\n",
        "                # Display answer with metadata if available\n",
        "                response_content = assistant_msg['content']\n",
        "                if isinstance(assistant_msg.get('metadata'), dict):\n",
        "                    meta = assistant_msg['metadata']\n",
        "                    context_indicator = \"üîÑ\" if meta.get('context_used', False) else \"üÜï\"\n",
        "                    display(Markdown(f\"**üìä Analysis:** {context_indicator} Type: `{meta.get('question_type', 'unknown')}` | Time: `{meta.get('processing_time', 0):.2f}s` | Confidence: {meta.get('confidence', 0):.0f}/100\"))\n",
        "\n",
        "                    # Show sub-question information if available (formatted)\n",
        "                    if meta.get('sub_questions_info'):\n",
        "                        # Parse and format sub-question information\n",
        "                        sub_info = meta['sub_questions_info']\n",
        "                        if 'Generated' in sub_info and 'sub questions' in sub_info:\n",
        "                            # Extract number of sub-questions\n",
        "                            import re\n",
        "                            match = re.search(r'Generated (\\d+) sub questions', sub_info)\n",
        "                            if match:\n",
        "                                num_questions = match.group(1)\n",
        "                                display(Markdown(f\"**üîç Query Processing:** Used multi-step reasoning with {num_questions} sub-questions\"))\n",
        "                        else:\n",
        "                            display(Markdown(f\"**üîç Query Processing:** {sub_info}\"))\n",
        "\n",
        "                display(Markdown(f\"**ü§ñ A:** {response_content}\"))\n",
        "\n",
        "                # Enhanced source citation with page numbers and sections\n",
        "                if isinstance(assistant_msg.get('metadata'), dict) and assistant_msg['metadata'].get('source_nodes'):\n",
        "                    source_nodes = assistant_msg['metadata']['source_nodes']\n",
        "                    if source_nodes:\n",
        "                        display(Markdown(\"**üìö Sources Referenced:**\"))\n",
        "                        for i, node in enumerate(source_nodes[:3], 1):  # Show top 3 sources\n",
        "                            # Extract source information\n",
        "                            source_meta = node.node.metadata\n",
        "                            page_info = source_meta.get('page_label', source_meta.get('source', 'Unknown'))\n",
        "\n",
        "                            # Get text preview\n",
        "                            text_preview = node.node.text[:120].replace('\\n', ' ').strip()\n",
        "\n",
        "                            # Format source citation\n",
        "                            if page_info != 'Unknown':\n",
        "                                display(Markdown(f\"**{i}.** Page {page_info}: *\\\"{text_preview}...\\\"*\"))\n",
        "                            else:\n",
        "                                display(Markdown(f\"**{i}.** Document Section: *\\\"{text_preview}...\\\"*\"))\n",
        "\n",
        "                display(Markdown(\"---\"))\n",
        "\n",
        "def enhanced_query_processing(question):\n",
        "    \"\"\"Enhanced query processing with better context handling\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Ensure we work with string input\n",
        "    question_str = str(question).strip()\n",
        "\n",
        "    # Step 1: Classify question type\n",
        "    question_type = classify_question(question_str)\n",
        "\n",
        "    # Step 2: Enhanced context handling using the enhanced history\n",
        "    if chat_history_v3_enhanced:\n",
        "        # Get last 2 exchanges for context\n",
        "        recent_history = chat_history_v3_enhanced[-4:]\n",
        "\n",
        "        # Detect follow-up questions\n",
        "        follow_up_indicators = [\n",
        "            'elaborate', 'explain more', 'tell me more', 'expand', 'details',\n",
        "            'that', 'it', 'this', 'further', 'more about', 'specific',\n",
        "            'can you', 'what about', 'how about'\n",
        "        ]\n",
        "        is_follow_up = any(indicator in question_str.lower() for indicator in follow_up_indicators)\n",
        "\n",
        "        if is_follow_up and len(recent_history) >= 2:\n",
        "            # Enhanced follow-up handling\n",
        "            last_question = recent_history[-2]['content'] if recent_history[-2]['role'] == 'user' else \"\"\n",
        "            last_answer = recent_history[-1]['content'] if recent_history[-1]['role'] == 'assistant' else \"\"\n",
        "\n",
        "            contextual_question = f\"\"\"Previous Question: {last_question}\n",
        "Previous Answer: {last_answer}\n",
        "\n",
        "User Follow-up Request: {question_str}\n",
        "\n",
        "Please provide more detailed information, elaborate further, or answer the follow-up question about the same topic.\"\"\"\n",
        "        else:\n",
        "            # Regular context for independent questions\n",
        "            context_str = \"\\n\".join([\n",
        "                f\"{msg['role'].title()}: {msg['content'][:100]}...\" if len(msg['content']) > 100 else f\"{msg['role'].title()}: {msg['content']}\"\n",
        "                for msg in recent_history\n",
        "            ])\n",
        "            contextual_question = f\"Context:\\n{context_str}\\n\\nNew Question: {question_str}\"\n",
        "    else:\n",
        "        contextual_question = question_str\n",
        "\n",
        "    # Step 3: Enhanced prompting for better content extraction\n",
        "    import sys\n",
        "    from io import StringIO\n",
        "    import contextlib\n",
        "\n",
        "    # Enhance the question for better content retrieval\n",
        "    enhanced_contextual_question = contextual_question\n",
        "\n",
        "    # For complex or summary questions, add specific instructions\n",
        "    if question_type in ['summary', 'comparison'] or len(question_str.split()) > 10:\n",
        "        enhanced_contextual_question = f\"\"\"{contextual_question}\n",
        "\n",
        "Please provide specific details including:\n",
        "- Exact timeframes, deadlines, and numerical values when mentioned\n",
        "- Specific document sections, page references, or policy numbers\n",
        "- Detailed procedures, requirements, and step-by-step processes\n",
        "- Concrete examples rather than general statements\n",
        "- Avoid generic advice like \"contact the company\" - extract specific policy information instead\n",
        "\n",
        "Focus on extracting precise information directly from the insurance policy document.\"\"\"\n",
        "\n",
        "    # Capture sub-question engine output\n",
        "    captured_output = StringIO()\n",
        "\n",
        "    with contextlib.redirect_stdout(captured_output):\n",
        "        if question_type in ['comparison', 'summary'] or len(question_str.split()) > 15:\n",
        "            response = sub_question_engine.query(enhanced_contextual_question)\n",
        "        else:\n",
        "            response = hybrid_query_engine.query(enhanced_contextual_question)\n",
        "\n",
        "    # Get and clean captured sub-question information\n",
        "    sub_questions_output = captured_output.getvalue()\n",
        "\n",
        "    # Clean and format the sub-question output\n",
        "    cleaned_sub_info = None\n",
        "    if sub_questions_output.strip():\n",
        "        # Remove extra whitespace and format\n",
        "        lines = [line.strip() for line in sub_questions_output.strip().split('\\n') if line.strip()]\n",
        "        if lines:\n",
        "            # Join meaningful lines\n",
        "            cleaned_sub_info = ' | '.join(lines[:3])  # Take first 3 meaningful lines\n",
        "\n",
        "    # Step 4: Calculate confidence\n",
        "    source_nodes = getattr(response, 'source_nodes', [])\n",
        "    confidence, factors = calculate_confidence_score(response.response, source_nodes)\n",
        "\n",
        "    processing_time = time.time() - start_time\n",
        "\n",
        "    return {\n",
        "        'response': response,\n",
        "        'question_type': question_type,\n",
        "        'confidence': confidence,\n",
        "        'factors': factors,\n",
        "        'processing_time': processing_time,\n",
        "        'source_nodes': source_nodes,\n",
        "        'context_used': len(chat_history_v3_enhanced) > 0,\n",
        "        'sub_questions_info': cleaned_sub_info\n",
        "    }\n",
        "\n",
        "def on_submit_enhanced(sender):\n",
        "    question = question_box_enhanced.value.strip()\n",
        "    if not question:\n",
        "        return\n",
        "\n",
        "    if question.lower() == 'exit':\n",
        "        question_box_enhanced.disabled = True\n",
        "        with output_area_enhanced:\n",
        "            clear_output()\n",
        "            display(Markdown(\"**üîö Chat session ended. Run the cell again to restart.**\"))\n",
        "        return\n",
        "\n",
        "    if question.lower() == 'clear':\n",
        "        # Clear all conversation histories\n",
        "        chat_history_v3_enhanced.clear()\n",
        "        # Also clear the regular v3 history used by other components\n",
        "        global chat_history_v3\n",
        "        chat_history_v3.clear()\n",
        "\n",
        "        # Clear ALL outputs including sub-question engine outputs\n",
        "        from IPython.display import clear_output as global_clear_output\n",
        "        global_clear_output(wait=True)\n",
        "\n",
        "        # Re-display the interface\n",
        "        display(Markdown(\"### üöÄ Enhanced RAG Chat (v3+)\\n*Features: Persistent History, Better Follow-ups, Scrollable Output*\"))\n",
        "        display(question_box_enhanced)\n",
        "        display(output_area_enhanced)\n",
        "\n",
        "        # Reset the display with cleared message\n",
        "        display_chat_history()\n",
        "        question_box_enhanced.value = ''\n",
        "\n",
        "        # Show confirmation message\n",
        "        with output_area_enhanced:\n",
        "            display(Markdown(\"‚úÖ **Conversation history cleared!** All context has been reset.\"))\n",
        "        return\n",
        "\n",
        "    # Add user question to history\n",
        "    chat_history_v3_enhanced.append({'role': 'user', 'content': question})\n",
        "\n",
        "    # Show processing message\n",
        "    with output_area_enhanced:\n",
        "        # Keep existing history and add processing message\n",
        "        display(Markdown(f\"**ü§î Q:** {question}\"))\n",
        "        display(Markdown(\"*üîÑ Processing with enhanced v3 features...*\"))\n",
        "\n",
        "    try:\n",
        "        # Process the question\n",
        "        result = enhanced_query_processing(question)\n",
        "\n",
        "        # Add assistant response with metadata to history\n",
        "        chat_history_v3_enhanced.append({\n",
        "            'role': 'assistant',\n",
        "            'content': result['response'].response,\n",
        "            'metadata': {\n",
        "                'question_type': result['question_type'],\n",
        "                'confidence': result['confidence'],\n",
        "                'processing_time': result['processing_time'],\n",
        "                'context_used': result['context_used'],\n",
        "                'sub_questions_info': result.get('sub_questions_info'),\n",
        "                'source_nodes': result.get('source_nodes', [])\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # Refresh the display with complete history\n",
        "        display_chat_history()\n",
        "\n",
        "    except Exception as e:\n",
        "        with output_area_enhanced:\n",
        "            display(Markdown(f\"**‚ùå Error:** {str(e)}\"))\n",
        "\n",
        "    question_box_enhanced.value = ''\n",
        "\n",
        "# Set up the interface\n",
        "question_box_enhanced.on_submit(on_submit_enhanced)\n",
        "\n",
        "# Display the enhanced interface\n",
        "display(Markdown(\"### üöÄ Enhanced RAG Chat (v3+)\\n*Features: Persistent History, Better Follow-ups, Scrollable Output*\"))\n",
        "display(question_box_enhanced)\n",
        "display(output_area_enhanced)\n",
        "\n",
        "# Initialize with welcome message\n",
        "display_chat_history()"
      ],
      "metadata": {
        "id": "Is2pkc2TRtiK",
        "outputId": "f966fbb2-55ca-4a56-87c3-316baae5205c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "8458f85938714ca8bd2f780991bce3a7",
            "80d2429e71ea408fac2d5656091c6294",
            "e73702e21d224f1283e8925b7a96ddb1",
            "63deb89e2283464a894d347b96d3afcc",
            "b768137fbb324c6db1afceb96d4d9aa1"
          ]
        }
      },
      "id": "Is2pkc2TRtiK",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### üöÄ Enhanced RAG Chat (v3+)\n*Features: Persistent History, Better Follow-ups, Scrollable Output*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='clear', description='Question:', layout=Layout(width='700px'), placeholder='Ask about your insuran‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8458f85938714ca8bd2f780991bce3a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output(layout=Layout(border='1px solid #ccc', height='400px', overflow_y='auto', width='100%'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63deb89e2283464a894d347b96d3afcc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LEKK7hwyTjfV"
      },
      "id": "LEKK7hwyTjfV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b649b85ffdc846ecbca1089dd6cccfe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Question:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8b1ae3e2796b4dafb0000155682120d3",
            "placeholder": "Type your question about the insurance policy...",
            "style": "IPY_MODEL_56f5924621df48c086317bc4679f967f",
            "value": ""
          }
        },
        "8b1ae3e2796b4dafb0000155682120d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56f5924621df48c086317bc4679f967f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f3fa9d262b641cdbc34d00c6dc76bf3": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_5fdfa157754d4cb78ba11b3b2aa6a3d0",
            "msg_id": "",
            "outputs": []
          }
        },
        "5fdfa157754d4cb78ba11b3b2aa6a3d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c2c7c137fc34632baa10ccbb6d10a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Question:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_35ce380b32504b14a5a55b134ad97b9a",
            "placeholder": "Ask about your insurance policy (Enhanced v3 with persistent history)...",
            "style": "IPY_MODEL_e074529b34f34b2cba39df2fd3a372c5",
            "value": ""
          }
        },
        "35ce380b32504b14a5a55b134ad97b9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "700px"
          }
        },
        "e074529b34f34b2cba39df2fd3a372c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a65482c7be8c4c2abf19e48041aaf43a": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_38caca798ccc479e8f77049b8a4f372c",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "### üí¨ Exchange 1"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**ü§î Q:** Summarize the policy exclusions, claim procedures, and premium payment schedule"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**üìä Analysis:** üîÑ Type: `procedural` | Time: `1.72s` | Confidence: 69/100"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**ü§ñ A:** The policy exclusions include no assignments of Member Life Insurance and limited rights for Dependents. Claim procedures involve notice of claim, claim forms, proof of loss, payment, denial, review, medical examinations, autopsy, legal action, and time limits. The premium payment schedule is determined based on the total volume of insurance in force divided by 1,000, multiplied by the premium rate then in effect for Member Life Insurance and Member Accidental Death and Dismemberment Insurance."
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**üìö Sources Referenced:**"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**1.** Page 6: *\"This policy has been updated effective January 1, 2014        GC 6001 TABLE OF CONTENTS, PAGE 1     TABLE OF CONTENTS...\"*"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**2.** Page 8: *\"This policy has been updated effective January 1, 2014        GC 6001 TABLE OF CONTENTS, PAGE 3          Section A - Mem...\"*"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**3.** Page 21: *\"This policy has been updated effective  January 1, 2014      PART II - POLICY ADMINISTRATION  GC 6004 Section B - Premiu...\"*"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "---"
                },
                "metadata": {}
              }
            ]
          }
        },
        "38caca798ccc479e8f77049b8a4f372c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid #ccc",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "400px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": "auto",
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "8458f85938714ca8bd2f780991bce3a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Question:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_80d2429e71ea408fac2d5656091c6294",
            "placeholder": "Ask about your insurance policy (Enhanced v3 with persistent history)...",
            "style": "IPY_MODEL_e73702e21d224f1283e8925b7a96ddb1",
            "value": ""
          }
        },
        "80d2429e71ea408fac2d5656091c6294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "700px"
          }
        },
        "e73702e21d224f1283e8925b7a96ddb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63deb89e2283464a894d347b96d3afcc": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_b768137fbb324c6db1afceb96d4d9aa1",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "### üí¨ Exchange 1"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**ü§î Q:** What would happen in a very unusual circumstance?"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**üìä Analysis:** üîÑ Type: `factual` | Time: `0.99s` | Confidence: 64/100"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**ü§ñ A:** In a very unusual circumstance, the policy would likely have specific provisions or guidelines in place to address such situations. These provisions would determine the course of action or benefits applicable based on the unique circumstances outlined in the policy."
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**üìö Sources Referenced:**"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**1.** Page 58: *\"This policy has been updated effective  January 1, 2014          PART IV - BENEFITS  GC 6015  Section B - Member Acciden...\"*"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**2.** Page 54: *\"Payment will be subject to the  Beneficiary, Facility of Payment and Settlement of Proceeds provisions of PART IV, Secti...\"*"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**3.** Page 31: *\"i. Effective Date for Benefit Changes Due to Change in the Member's Family Status    A Member may request an increase in...\"*"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "---"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "### üí¨ Exchange 2"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**ü§î Q:** What are my options if I want to cancel?"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**üìä Analysis:** üîÑ Type: `factual` | Time: `0.97s` | Confidence: 59/100"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**ü§ñ A:** If you want to cancel, you can terminate the Group Policy effective on the day before any premium due date by providing written notice to The Principal before that premium due date. Additionally, issuing a stop-payment order for any amounts used to pay premiums for your coverage will also be considered written notice for cancellation."
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**üìö Sources Referenced:**"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**1.** Page 37: *\"This policy has been updated effective  January 1, 2014      PART III - INDIVIDUAL REQUIREMENTS AND RIGHTS  GC 6008 Sect...\"*"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**2.** Page 35: *\"This policy has been updated effective  January 1, 2014      PART III - INDIVIDUAL REQUIREMENTS AND RIGHTS  GC 6008 Sect...\"*"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**3.** Page 43: *\"This policy has been updated effective  January 1, 2014    PART III - INDIVIDUAL REQUIREMENTS AND RIGHTS  GC 6011  Secti...\"*"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "---"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "### üí¨ Exchange 3"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**ü§î Q:** How much is the death benefit?"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**üìä Analysis:** üîÑ Type: `procedural` | Time: `1.90s` | Confidence: 63/100"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**ü§ñ A:** The death benefit amount payable under the policy is based on the Scheduled Benefit (or approved amount, if applicable) in force for the Member at the time of their death. This benefit amount may be subject to adjustments or deductions based on specific circumstances outlined in the policy, such as any Accelerated Benefit payments made or individual policy amounts purchased earlier. The policy also specifies the order of precedence for payment if no beneficiary survives the Member or if no beneficiary has been named. Additionally, the policy outlines conditions under which the Death Benefits Payable may be withheld until further information is received or a trial is held."
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**üìö Sources Referenced:**"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**1.** Page 45: *\"This policy has been updated effective  January 1, 2014    PART III - INDIVIDUAL REQUIREMENTS AND RIGHTS  GC 6011  Secti...\"*"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**2.** Page 48: *\"This policy has been updated effective  January 1, 2014      PART IV - BENEFITS  GC 6013  Section A - Member Life Insura...\"*"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "**3.** Page 47: *\"a. If a beneficiary is found guilty of the Member's death, such beneficiary may be disqualified  from receiving any bene...\"*"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "---"
                },
                "metadata": {}
              }
            ]
          }
        },
        "b768137fbb324c6db1afceb96d4d9aa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid #ccc",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "400px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": "auto",
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}