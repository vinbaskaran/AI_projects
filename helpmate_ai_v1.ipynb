{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5646004c",
   "metadata": {},
   "source": [
    "# Efficient RAG System for Insurance Documents\n",
    "\n",
    "## Overview\n",
    "This notebook implements an optimized Retrieval-Augmented Generation (RAG) system for insurance document querying using:\n",
    "- **OpenAI Embeddings** for semantic understanding\n",
    "- **ChromaDB** for efficient vector storage and retrieval\n",
    "- **Cross-encoder re-ranking** for improved result relevance\n",
    "- **GPT-3.5** for natural language response generation\n",
    "\n",
    "### Key Improvements Over Original Implementation:\n",
    "1. **Modular Architecture**: Separate classes for each component\n",
    "2. **Error Handling**: Comprehensive error handling and logging\n",
    "3. **Performance Optimization**: Batch processing and caching strategies\n",
    "4. **Configuration Management**: Centralized configuration system\n",
    "5. **Resource Management**: Efficient memory and API usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cfb8ab",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies with version pinning for reproducibility\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q pdfplumber==0.10.3 \\\n",
    "                tiktoken==0.5.2 \\\n",
    "                openai==1.3.8 \\\n",
    "                chromadb==0.4.18 \\\n",
    "                sentence-transformers==2.2.2 \\\n",
    "                pandas==2.1.4 \\\n",
    "                numpy==1.24.3 \\\n",
    "                tqdm==4.66.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from operator import itemgetter\n",
    "\n",
    "# PDF processing\n",
    "import pdfplumber\n",
    "\n",
    "# NLP and ML libraries\n",
    "import tiktoken\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"✅ All dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f240220",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration class for the RAG system\"\"\"\n",
    "    \n",
    "    # API Configuration\n",
    "    openai_api_key: str = \"\"\n",
    "    embedding_model: str = \"text-embedding-ada-002\"\n",
    "    chat_model: str = \"gpt-3.5-turbo\"\n",
    "    \n",
    "    # File paths\n",
    "    pdf_directory: str = \"./documents\"\n",
    "    chroma_persist_directory: str = \"./chroma_db\"\n",
    "    \n",
    "    # Processing parameters\n",
    "    min_text_length: int = 10\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 200\n",
    "    \n",
    "    # Search parameters\n",
    "    similarity_threshold: float = 0.2\n",
    "    max_search_results: int = 10\n",
    "    top_k_rerank: int = 3\n",
    "    \n",
    "    # Cross-encoder model\n",
    "    cross_encoder_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    \n",
    "    # Collection names\n",
    "    main_collection_name: str = \"insurance_documents\"\n",
    "    cache_collection_name: str = \"query_cache\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration after initialization\"\"\"\n",
    "        if not self.openai_api_key:\n",
    "            raise ValueError(\"OpenAI API key is required\")\n",
    "        \n",
    "        # Set OpenAI API key\n",
    "        openai.api_key = self.openai_api_key\n",
    "\n",
    "# Initialize configuration\n",
    "try:\n",
    "    # Try to load API key from environment or file\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not api_key:\n",
    "        # For local development, you can create a file with your API key\n",
    "        try:\n",
    "            with open('openai_key.txt', 'r') as f:\n",
    "                api_key = f.read().strip()\n",
    "        except FileNotFoundError:\n",
    "            api_key = input(\"Please enter your OpenAI API key: \")\n",
    "    \n",
    "    config = RAGConfig(openai_api_key=api_key)\n",
    "    print(\"✅ Configuration initialized successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Configuration initialization failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a2d66",
   "metadata": {},
   "source": [
    "## 2. Data Processing Pipeline for PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFProcessor:\n",
    "    \"\"\"Optimized PDF processing with batch capabilities and error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    @staticmethod\n",
    "    def check_bboxes(word: Dict, table_bbox: Tuple) -> bool:\n",
    "        \"\"\"Check if word is inside a table bounding box\"\"\"\n",
    "        word_bbox = (word['x0'], word['top'], word['x1'], word['bottom'])\n",
    "        return (word_bbox[0] > table_bbox[0] and word_bbox[1] > table_bbox[1] and \n",
    "                word_bbox[2] < table_bbox[2] and word_bbox[3] < table_bbox[3])\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: Path) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Extract text and tables from PDF with error handling\"\"\"\n",
    "        pages_data = []\n",
    "        \n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, 1):\n",
    "                    try:\n",
    "                        # Extract tables and their bounding boxes\n",
    "                        tables = page.find_tables()\n",
    "                        table_bboxes = [table.bbox for table in tables]\n",
    "                        table_data = [{'table': table.extract(), 'top': table.bbox[1]} \n",
    "                                    for table in tables]\n",
    "                        \n",
    "                        # Extract non-table words\n",
    "                        words = page.extract_words()\n",
    "                        non_table_words = [\n",
    "                            word for word in words \n",
    "                            if not any(self.check_bboxes(word, bbox) for bbox in table_bboxes)\n",
    "                        ]\n",
    "                        \n",
    "                        # Combine text and tables in reading order\n",
    "                        all_elements = non_table_words + table_data\n",
    "                        if all_elements:\n",
    "                            clusters = pdfplumber.utils.cluster_objects(\n",
    "                                all_elements, itemgetter('top'), tolerance=5\n",
    "                            )\n",
    "                            \n",
    "                            lines = []\n",
    "                            for cluster in clusters:\n",
    "                                if cluster and 'text' in cluster[0]:\n",
    "                                    try:\n",
    "                                        line_text = ' '.join(item['text'] for item in cluster)\n",
    "                                        lines.append(line_text)\n",
    "                                    except (KeyError, TypeError):\n",
    "                                        continue\n",
    "                                elif cluster and 'table' in cluster[0]:\n",
    "                                    table_json = json.dumps(cluster[0]['table'])\n",
    "                                    lines.append(table_json)\n",
    "                            \n",
    "                            page_text = ' '.join(lines)\n",
    "                            if len(page_text.split()) >= self.config.min_text_length:\n",
    "                                pages_data.append((f\"Page {page_num}\", page_text))\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error processing page {page_num} of {pdf_path.name}: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error opening PDF {pdf_path.name}: {e}\")\n",
    "            return []\n",
    "            \n",
    "        return pages_data\n",
    "    \n",
    "    def process_directory(self, directory_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Process all PDFs in directory with progress tracking\"\"\"\n",
    "        pdf_dir = Path(directory_path)\n",
    "        if not pdf_dir.exists():\n",
    "            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n",
    "            \n",
    "        pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "        if not pdf_files:\n",
    "            logger.warning(f\"No PDF files found in {directory_path}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        all_data = []\n",
    "        \n",
    "        with tqdm(pdf_files, desc=\"Processing PDFs\") as pbar:\n",
    "            for pdf_path in pbar:\n",
    "                pbar.set_postfix({\"Current\": pdf_path.name})\n",
    "                \n",
    "                try:\n",
    "                    pages_data = self.extract_text_from_pdf(pdf_path)\n",
    "                    \n",
    "                    for page_no, page_text in pages_data:\n",
    "                        all_data.append({\n",
    "                            'Page_No': page_no,\n",
    "                            'Page_Text': page_text,\n",
    "                            'Document_Name': pdf_path.name,\n",
    "                            'Text_Length': len(page_text.split()),\n",
    "                            'Policy_Name': pdf_path.stem,\n",
    "                            'Source_Path': str(pdf_path)\n",
    "                        })\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to process {pdf_path.name}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if not all_data:\n",
    "            logger.warning(\"No data extracted from PDFs\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        df = pd.DataFrame(all_data)\n",
    "        \n",
    "        # Filter out pages with insufficient content\n",
    "        initial_count = len(df)\n",
    "        df = df[df['Text_Length'] >= self.config.min_text_length]\n",
    "        filtered_count = len(df)\n",
    "        \n",
    "        logger.info(f\"Processed {len(pdf_files)} PDFs, extracted {initial_count} pages, \"\n",
    "                   f\"kept {filtered_count} pages after filtering\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Initialize PDF processor\n",
    "pdf_processor = PDFProcessor(config)\n",
    "print(\"✅ PDF processor initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43e1f4",
   "metadata": {},
   "source": [
    "## 3. Vector Database Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf22a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDatabaseManager:\n",
    "    \"\"\"Manages ChromaDB collections with connection pooling and error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.client = None\n",
    "        self.embedding_function = None\n",
    "        self.main_collection = None\n",
    "        self.cache_collection = None\n",
    "        \n",
    "    def initialize_client(self):\n",
    "        \"\"\"Initialize ChromaDB client with persistence\"\"\"\n",
    "        try:\n",
    "            # Create persist directory if it doesn't exist\n",
    "            persist_dir = Path(self.config.chroma_persist_directory)\n",
    "            persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            self.client = chromadb.PersistentClient(path=str(persist_dir))\n",
    "            \n",
    "            # Initialize embedding function\n",
    "            self.embedding_function = OpenAIEmbeddingFunction(\n",
    "                api_key=self.config.openai_api_key,\n",
    "                model_name=self.config.embedding_model\n",
    "            )\n",
    "            \n",
    "            logger.info(\"ChromaDB client initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize ChromaDB client: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_or_create_collections(self):\n",
    "        \"\"\"Create or retrieve existing collections\"\"\"\n",
    "        if not self.client:\n",
    "            self.initialize_client()\n",
    "            \n",
    "        try:\n",
    "            # Main documents collection\n",
    "            self.main_collection = self.client.get_or_create_collection(\n",
    "                name=self.config.main_collection_name,\n",
    "                embedding_function=self.embedding_function\n",
    "            )\n",
    "            \n",
    "            # Cache collection for query optimization\n",
    "            self.cache_collection = self.client.get_or_create_collection(\n",
    "                name=self.config.cache_collection_name,\n",
    "                embedding_function=self.embedding_function\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Collections initialized: \"\n",
    "                       f\"Main({self.main_collection.count()} docs), \"\n",
    "                       f\"Cache({self.cache_collection.count()} queries)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create collections: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def add_documents_batch(self, documents_df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"Add documents to collection in batches for efficiency\"\"\"\n",
    "        if self.main_collection is None:\n",
    "            self.get_or_create_collections()\n",
    "        \n",
    "        documents = documents_df['Page_Text'].tolist()\n",
    "        metadatas = documents_df[['Policy_Name', 'Page_No', 'Document_Name', 'Text_Length']].to_dict('records')\n",
    "        ids = [f\"{row['Policy_Name']}_{row['Page_No']}\" for _, row in documents_df.iterrows()]\n",
    "        \n",
    "        # Process in batches to avoid memory issues\n",
    "        total_batches = len(documents) // batch_size + (1 if len(documents) % batch_size else 0)\n",
    "        \n",
    "        with tqdm(total=total_batches, desc=\"Adding documents to vector DB\") as pbar:\n",
    "            for i in range(0, len(documents), batch_size):\n",
    "                batch_docs = documents[i:i + batch_size]\n",
    "                batch_metadata = metadatas[i:i + batch_size]\n",
    "                batch_ids = ids[i:i + batch_size]\n",
    "                \n",
    "                try:\n",
    "                    self.main_collection.add(\n",
    "                        documents=batch_docs,\n",
    "                        metadatas=batch_metadata,\n",
    "                        ids=batch_ids\n",
    "                    )\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error adding batch {i//batch_size + 1}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        final_count = self.main_collection.count()\n",
    "        logger.info(f\"Successfully added documents. Total in collection: {final_count}\")\n",
    "        \n",
    "        return final_count\n",
    "    \n",
    "    def check_collection_status(self):\n",
    "        \"\"\"Check the status of collections\"\"\"\n",
    "        if not self.client:\n",
    "            return \"Client not initialized\"\n",
    "            \n",
    "        try:\n",
    "            collections = self.client.list_collections()\n",
    "            status = {\n",
    "                \"total_collections\": len(collections),\n",
    "                \"collection_names\": [col.name for col in collections]\n",
    "            }\n",
    "            \n",
    "            if self.main_collection:\n",
    "                status[\"main_collection_count\"] = self.main_collection.count()\n",
    "            if self.cache_collection:\n",
    "                status[\"cache_collection_count\"] = self.cache_collection.count()\n",
    "                \n",
    "            return status\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking collection status: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Initialize vector database manager\n",
    "vector_db = VectorDatabaseManager(config)\n",
    "vector_db.initialize_client()\n",
    "vector_db.get_or_create_collections()\n",
    "\n",
    "print(\"✅ Vector database manager initialized!\")\n",
    "print(f\"Status: {vector_db.check_collection_status()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3604b5",
   "metadata": {},
   "source": [
    "## 4. Semantic Search with Intelligent Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ded44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearchEngine:\n",
    "    \"\"\"Optimized semantic search with intelligent caching and performance metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_db: VectorDatabaseManager, config: RAGConfig):\n",
    "        self.vector_db = vector_db\n",
    "        self.config = config\n",
    "        self.search_metrics = {\n",
    "            \"total_searches\": 0,\n",
    "            \"cache_hits\": 0,\n",
    "            \"cache_misses\": 0,\n",
    "            \"average_search_time\": 0\n",
    "        }\n",
    "    \n",
    "    def search_cache(self, query: str) -> Optional[Dict]:\n",
    "        \"\"\"Search for similar queries in cache\"\"\"\n",
    "        try:\n",
    "            cache_results = self.vector_db.cache_collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=1\n",
    "            )\n",
    "            \n",
    "            if (cache_results['distances'] and \n",
    "                cache_results['distances'][0] and \n",
    "                cache_results['distances'][0][0] <= self.config.similarity_threshold):\n",
    "                \n",
    "                self.search_metrics[\"cache_hits\"] += 1\n",
    "                logger.info(f\"Cache hit for query: {query[:50]}...\")\n",
    "                return self._parse_cache_results(cache_results)\n",
    "            else:\n",
    "                self.search_metrics[\"cache_misses\"] += 1\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cache search error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _parse_cache_results(self, cache_results: Dict) -> Dict:\n",
    "        \"\"\"Parse cached search results\"\"\"\n",
    "        try:\n",
    "            metadata = cache_results['metadatas'][0][0]\n",
    "            \n",
    "            # Reconstruct results from cache metadata\n",
    "            results = {\n",
    "                'documents': [],\n",
    "                'metadatas': [],\n",
    "                'distances': [],\n",
    "                'ids': []\n",
    "            }\n",
    "            \n",
    "            for key, value in metadata.items():\n",
    "                if key.startswith('documents'):\n",
    "                    results['documents'].append(value)\n",
    "                elif key.startswith('metadatas'):\n",
    "                    results['metadatas'].append(eval(value))  # Note: Use json.loads in production\n",
    "                elif key.startswith('distances'):\n",
    "                    results['distances'].append(float(value))\n",
    "                elif key.startswith('ids'):\n",
    "                    results['ids'].append(value)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing cache results: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def search_main_collection(self, query: str) -> Dict:\n",
    "        \"\"\"Search main document collection\"\"\"\n",
    "        try:\n",
    "            results = self.vector_db.main_collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=self.config.max_search_results\n",
    "            )\n",
    "            \n",
    "            # Cache the results for future use\n",
    "            self._cache_search_results(query, results)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Main collection search error: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _cache_search_results(self, query: str, results: Dict):\n",
    "        \"\"\"Cache search results for future queries\"\"\"\n",
    "        try:\n",
    "            # Prepare metadata for caching\n",
    "            cache_metadata = {}\n",
    "            \n",
    "            for i, (doc, meta, dist, doc_id) in enumerate(zip(\n",
    "                results.get('documents', [[]])[0],\n",
    "                results.get('metadatas', [[]])[0], \n",
    "                results.get('distances', [[]])[0],\n",
    "                results.get('ids', [[]])[0]\n",
    "            )):\n",
    "                cache_metadata[f'documents{i}'] = doc\n",
    "                cache_metadata[f'metadatas{i}'] = str(meta)\n",
    "                cache_metadata[f'distances{i}'] = str(dist)\n",
    "                cache_metadata[f'ids{i}'] = doc_id\n",
    "            \n",
    "            # Add to cache collection\n",
    "            self.vector_db.cache_collection.add(\n",
    "                documents=[query],\n",
    "                ids=[f\"query_{hash(query)}\"],\n",
    "                metadatas=[cache_metadata]\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error caching results: {e}\")\n",
    "    \n",
    "    def search(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Perform semantic search with caching\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.search_metrics[\"total_searches\"] += 1\n",
    "        \n",
    "        # Check cache first\n",
    "        cached_results = self.search_cache(query)\n",
    "        \n",
    "        if cached_results:\n",
    "            results = cached_results\n",
    "            logger.info(\"Returning cached results\")\n",
    "        else:\n",
    "            # Search main collection\n",
    "            results = self.search_main_collection(query)\n",
    "            logger.info(\"Returning fresh search results\")\n",
    "        \n",
    "        # Convert to DataFrame for easier handling\n",
    "        if results and 'documents' in results and results['documents']:\n",
    "            results_df = pd.DataFrame({\n",
    "                'Documents': results['documents'][0],\n",
    "                'Metadatas': results['metadatas'][0],\n",
    "                'Distances': results['distances'][0],\n",
    "                'IDs': results['ids'][0]\n",
    "            })\n",
    "        else:\n",
    "            logger.warning(\"No search results found\")\n",
    "            results_df = pd.DataFrame()\n",
    "        \n",
    "        # Update metrics\n",
    "        search_time = time.time() - start_time\n",
    "        self.search_metrics[\"average_search_time\"] = (\n",
    "            (self.search_metrics[\"average_search_time\"] * (self.search_metrics[\"total_searches\"] - 1) + search_time) \n",
    "            / self.search_metrics[\"total_searches\"]\n",
    "        )\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def get_search_metrics(self) -> Dict:\n",
    "        \"\"\"Get search performance metrics\"\"\"\n",
    "        if self.search_metrics[\"total_searches\"] > 0:\n",
    "            cache_hit_rate = (self.search_metrics[\"cache_hits\"] / \n",
    "                            self.search_metrics[\"total_searches\"]) * 100\n",
    "        else:\n",
    "            cache_hit_rate = 0\n",
    "            \n",
    "        return {\n",
    "            **self.search_metrics,\n",
    "            \"cache_hit_rate\": f\"{cache_hit_rate:.2f}%\"\n",
    "        }\n",
    "\n",
    "# Initialize semantic search engine\n",
    "search_engine = SemanticSearchEngine(vector_db, config)\n",
    "print(\"✅ Semantic search engine initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f9fdb",
   "metadata": {},
   "source": [
    "## 5. Cross-Encoder Re-ranking Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcaa737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoderReranker:\n",
    "    \"\"\"Cross-encoder re-ranking with batch processing and performance comparison\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.load_model()\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load cross-encoder model with error handling\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading cross-encoder model: {self.config.cross_encoder_model}\")\n",
    "            self.model = CrossEncoder(self.config.cross_encoder_model)\n",
    "            logger.info(\"Cross-encoder model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load cross-encoder model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def rerank_results(self, query: str, search_results: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Re-rank search results using cross-encoder\"\"\"\n",
    "        if search_results.empty or self.model is None:\n",
    "            return search_results\n",
    "            \n",
    "        try:\n",
    "            # Prepare query-document pairs\n",
    "            query_doc_pairs = [\n",
    "                [query, doc] for doc in search_results['Documents']\n",
    "            ]\n",
    "            \n",
    "            # Get re-ranking scores\n",
    "            rerank_scores = self.model.predict(query_doc_pairs)\n",
    "            \n",
    "            # Add scores to dataframe\n",
    "            results_with_rerank = search_results.copy()\n",
    "            results_with_rerank['Rerank_Score'] = rerank_scores\n",
    "            \n",
    "            # Sort by re-ranking score (highest first)\n",
    "            results_with_rerank = results_with_rerank.sort_values(\n",
    "                'Rerank_Score', ascending=False\n",
    "            ).reset_index(drop=True)\n",
    "            \n",
    "            return results_with_rerank\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Re-ranking failed: {e}\")\n",
    "            return search_results\n",
    "    \n",
    "    def compare_rankings(self, query: str, search_results: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Compare semantic search vs re-ranked results\"\"\"\n",
    "        if search_results.empty:\n",
    "            return {\"error\": \"No results to compare\"}\n",
    "            \n",
    "        try:\n",
    "            # Get top results from semantic search (by distance)\n",
    "            semantic_top = search_results.nsmallest(self.config.top_k_rerank, 'Distances')\n",
    "            \n",
    "            # Get re-ranked results\n",
    "            reranked_results = self.rerank_results(query, search_results)\n",
    "            rerank_top = reranked_results.head(self.config.top_k_rerank)\n",
    "            \n",
    "            # Calculate overlap\n",
    "            semantic_ids = set(semantic_top['IDs'])\n",
    "            rerank_ids = set(rerank_top['IDs'])\n",
    "            overlap = len(semantic_ids.intersection(rerank_ids))\n",
    "            \n",
    "            comparison = {\n",
    "                \"semantic_top_distances\": semantic_top['Distances'].tolist(),\n",
    "                \"rerank_top_scores\": rerank_top['Rerank_Score'].tolist() if 'Rerank_Score' in rerank_top.columns else [],\n",
    "                \"overlap_count\": overlap,\n",
    "                \"overlap_percentage\": (overlap / self.config.top_k_rerank) * 100,\n",
    "                \"semantic_ids\": list(semantic_ids),\n",
    "                \"rerank_ids\": list(rerank_ids)\n",
    "            }\n",
    "            \n",
    "            return comparison\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ranking comparison failed: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def get_top_results(self, query: str, search_results: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Get top K results after re-ranking\"\"\"\n",
    "        reranked = self.rerank_results(query, search_results)\n",
    "        top_results = reranked.head(self.config.top_k_rerank)\n",
    "        \n",
    "        # Select relevant columns for RAG\n",
    "        if not top_results.empty:\n",
    "            return top_results[['Documents', 'Metadatas', 'Rerank_Score']].copy()\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Initialize cross-encoder reranker\n",
    "reranker = CrossEncoderReranker(config)\n",
    "print(\"✅ Cross-encoder reranker initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f739cf",
   "metadata": {},
   "source": [
    "## 6. Retrieval Augmented Generation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b24dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGGenerator:\n",
    "    \"\"\"Comprehensive RAG system with OpenAI integration and optimized prompting\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.system_prompt = self._create_system_prompt()\n",
    "        \n",
    "    def _create_system_prompt(self) -> str:\n",
    "        \"\"\"Create optimized system prompt for insurance domain\"\"\"\n",
    "        return \"\"\"You are a highly knowledgeable insurance specialist assistant. Your role is to provide accurate, helpful, and comprehensive answers about insurance policies and documents.\n",
    "\n",
    "Key Guidelines:\n",
    "1. Always base your answers on the provided document context\n",
    "2. Provide specific details including numbers, percentages, and policy terms when available\n",
    "3. If information is incomplete, clearly state what's missing and suggest where to find it\n",
    "4. Use clear, professional language that customers can understand\n",
    "5. Always include proper citations with policy names and page numbers\n",
    "6. If tables are present in the context, format them clearly in your response\n",
    "7. Focus only on information relevant to the user's question\"\"\"\n",
    "\n",
    "    def _create_user_prompt(self, query: str, context_documents: pd.DataFrame) -> str:\n",
    "        \"\"\"Create optimized user prompt with context\"\"\"\n",
    "        if context_documents.empty:\n",
    "            return f\"Query: {query}\\n\\nNo relevant documents found. Please inform the user that no information is available for their query.\"\n",
    "            \n",
    "        # Format context documents\n",
    "        context_text = \"\"\n",
    "        for idx, row in context_documents.iterrows():\n",
    "            metadata = row['Metadatas']\n",
    "            policy_name = metadata.get('Policy_Name', 'Unknown Policy')\n",
    "            page_no = metadata.get('Page_No', 'Unknown Page')\n",
    "            \n",
    "            context_text += f\"\\n--- Document {idx + 1} ---\\n\"\n",
    "            context_text += f\"Source: {policy_name}, {page_no}\\n\"\n",
    "            context_text += f\"Content: {row['Documents'][:2000]}...\\n\"  # Limit context length\n",
    "        \n",
    "        user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "Context Documents:\n",
    "{context_text}\n",
    "\n",
    "Instructions:\n",
    "1. Answer the user's query using ONLY the information from the context documents above\n",
    "2. Include specific details, numbers, and policy terms when available\n",
    "3. If tables are mentioned, reformat them clearly\n",
    "4. Provide citations in the format: [Policy Name - Page Number]\n",
    "5. If the query cannot be fully answered with the available context, state what information is missing\n",
    "6. Be concise but comprehensive\n",
    "\n",
    "Response:\"\"\"\n",
    "        \n",
    "        return user_prompt\n",
    "    \n",
    "    def generate_response(self, query: str, context_documents: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Generate response using OpenAI with error handling and retries\"\"\"\n",
    "        try:\n",
    "            user_prompt = self._create_user_prompt(query, context_documents)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            # Call OpenAI API with retry logic\n",
    "            response = self._call_openai_with_retry(messages)\n",
    "            \n",
    "            # Parse and format response\n",
    "            return self._format_response(response, query, context_documents)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Response generation failed: {e}\")\n",
    "            return {\n",
    "                \"response\": f\"I apologize, but I encountered an error while processing your query: {str(e)}\",\n",
    "                \"query\": query,\n",
    "                \"sources\": [],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _call_openai_with_retry(self, messages: List[Dict], max_retries: int = 3) -> str:\n",
    "        \"\"\"Call OpenAI API with retry logic\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = openai.chat.completions.create(\n",
    "                    model=self.config.chat_model,\n",
    "                    messages=messages,\n",
    "                    temperature=0.3,\n",
    "                    max_tokens=1500\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise e\n",
    "                logger.warning(f\"OpenAI API call failed (attempt {attempt + 1}): {e}\")\n",
    "                import time\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    \n",
    "    def _format_response(self, response_text: str, query: str, context_docs: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Format the response with metadata\"\"\"\n",
    "        sources = []\n",
    "        \n",
    "        if not context_docs.empty:\n",
    "            for _, row in context_docs.iterrows():\n",
    "                metadata = row['Metadatas']\n",
    "                sources.append({\n",
    "                    \"policy_name\": metadata.get('Policy_Name', 'Unknown'),\n",
    "                    \"page_number\": metadata.get('Page_No', 'Unknown'),\n",
    "                    \"relevance_score\": row.get('Rerank_Score', row.get('Distances', 0))\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"response\": response_text,\n",
    "            \"query\": query,\n",
    "            \"sources\": sources,\n",
    "            \"source_count\": len(sources),\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Initialize RAG generator\n",
    "rag_generator = RAGGenerator(config)\n",
    "print(\"✅ RAG generator initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752792fa",
   "metadata": {},
   "source": [
    "## 7. Complete RAG Pipeline Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientRAGPipeline:\n",
    "    \"\"\"Complete RAG pipeline orchestrator with performance monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.pdf_processor = PDFProcessor(config)\n",
    "        self.vector_db = VectorDatabaseManager(config)\n",
    "        self.search_engine = SemanticSearchEngine(self.vector_db, config)\n",
    "        self.reranker = CrossEncoderReranker(config)\n",
    "        self.rag_generator = RAGGenerator(config)\n",
    "        \n",
    "        self.pipeline_metrics = {\n",
    "            \"queries_processed\": 0,\n",
    "            \"average_response_time\": 0,\n",
    "            \"successful_responses\": 0,\n",
    "            \"failed_responses\": 0\n",
    "        }\n",
    "        \n",
    "        # Initialize components\n",
    "        self._initialize_pipeline()\n",
    "    \n",
    "    def _initialize_pipeline(self):\n",
    "        \"\"\"Initialize all pipeline components\"\"\"\n",
    "        logger.info(\"Initializing RAG pipeline components...\")\n",
    "        \n",
    "        try:\n",
    "            # Initialize vector database\n",
    "            self.vector_db.initialize_client()\n",
    "            self.vector_db.get_or_create_collections()\n",
    "            \n",
    "            logger.info(\"✅ RAG pipeline initialized successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline initialization failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def process_documents(self, pdf_directory: str = None) -> bool:\n",
    "        \"\"\"Process PDFs and populate vector database\"\"\"\n",
    "        try:\n",
    "            pdf_dir = pdf_directory or self.config.pdf_directory\n",
    "            \n",
    "            if not Path(pdf_dir).exists():\n",
    "                logger.error(f\"PDF directory not found: {pdf_dir}\")\n",
    "                return False\n",
    "            \n",
    "            # Extract text from PDFs\n",
    "            logger.info(\"Processing PDF documents...\")\n",
    "            documents_df = self.pdf_processor.process_directory(pdf_dir)\n",
    "            \n",
    "            if documents_df.empty:\n",
    "                logger.warning(\"No documents processed\")\n",
    "                return False\n",
    "            \n",
    "            # Add to vector database\n",
    "            logger.info(\"Adding documents to vector database...\")\n",
    "            doc_count = self.vector_db.add_documents_batch(documents_df)\n",
    "            \n",
    "            logger.info(f\"✅ Successfully processed {len(documents_df)} pages from {pdf_dir}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Document processing failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def query(self, user_query: str, enable_reranking: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Process a complete query through the RAG pipeline\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            self.pipeline_metrics[\"queries_processed\"] += 1\n",
    "            \n",
    "            # Step 1: Semantic search with caching\n",
    "            logger.info(f\"Processing query: {user_query[:100]}...\")\n",
    "            search_results = self.search_engine.search(user_query)\n",
    "            \n",
    "            if search_results.empty:\n",
    "                return {\n",
    "                    \"response\": \"I couldn't find any relevant information for your query. Please try rephrasing your question or check if the documents are properly loaded.\",\n",
    "                    \"query\": user_query,\n",
    "                    \"sources\": [],\n",
    "                    \"search_results_count\": 0,\n",
    "                    \"processing_time\": time.time() - start_time\n",
    "                }\n",
    "            \n",
    "            # Step 2: Re-ranking (optional)\n",
    "            if enable_reranking:\n",
    "                logger.info(\"Re-ranking search results...\")\n",
    "                top_results = self.reranker.get_top_results(user_query, search_results)\n",
    "            else:\n",
    "                # Use top results from semantic search\n",
    "                top_results = search_results.head(self.config.top_k_rerank)\n",
    "                top_results = top_results[['Documents', 'Metadatas']].copy()\n",
    "            \n",
    "            # Step 3: Generate response\n",
    "            logger.info(\"Generating response...\")\n",
    "            response_data = self.rag_generator.generate_response(user_query, top_results)\n",
    "            \n",
    "            # Add pipeline metrics\n",
    "            processing_time = time.time() - start_time\n",
    "            response_data.update({\n",
    "                \"search_results_count\": len(search_results),\n",
    "                \"processing_time\": processing_time,\n",
    "                \"reranking_enabled\": enable_reranking\n",
    "            })\n",
    "            \n",
    "            # Update metrics\n",
    "            self._update_metrics(processing_time, success=True)\n",
    "            \n",
    "            return response_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Query processing failed: {e}\")\n",
    "            self._update_metrics(time.time() - start_time, success=False)\n",
    "            \n",
    "            return {\n",
    "                \"response\": f\"I encountered an error while processing your query: {str(e)}\",\n",
    "                \"query\": user_query,\n",
    "                \"sources\": [],\n",
    "                \"error\": str(e),\n",
    "                \"processing_time\": time.time() - start_time\n",
    "            }\n",
    "    \n",
    "    def _update_metrics(self, processing_time: float, success: bool):\n",
    "        \"\"\"Update pipeline performance metrics\"\"\"\n",
    "        if success:\n",
    "            self.pipeline_metrics[\"successful_responses\"] += 1\n",
    "        else:\n",
    "            self.pipeline_metrics[\"failed_responses\"] += 1\n",
    "        \n",
    "        # Update average response time\n",
    "        total_queries = self.pipeline_metrics[\"queries_processed\"]\n",
    "        current_avg = self.pipeline_metrics[\"average_response_time\"]\n",
    "        self.pipeline_metrics[\"average_response_time\"] = (\n",
    "            (current_avg * (total_queries - 1) + processing_time) / total_queries\n",
    "        )\n",
    "    \n",
    "    def get_system_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive system status\"\"\"\n",
    "        try:\n",
    "            return {\n",
    "                \"pipeline_metrics\": self.pipeline_metrics,\n",
    "                \"search_metrics\": self.search_engine.get_search_metrics(),\n",
    "                \"vector_db_status\": self.vector_db.check_collection_status(),\n",
    "                \"config\": {\n",
    "                    \"embedding_model\": self.config.embedding_model,\n",
    "                    \"chat_model\": self.config.chat_model,\n",
    "                    \"similarity_threshold\": self.config.similarity_threshold,\n",
    "                    \"max_search_results\": self.config.max_search_results,\n",
    "                    \"top_k_rerank\": self.config.top_k_rerank\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Failed to get system status: {e}\"}\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Reset all performance metrics\"\"\"\n",
    "        self.pipeline_metrics = {\n",
    "            \"queries_processed\": 0,\n",
    "            \"average_response_time\": 0,\n",
    "            \"successful_responses\": 0,\n",
    "            \"failed_responses\": 0\n",
    "        }\n",
    "        self.search_engine.search_metrics = {\n",
    "            \"total_searches\": 0,\n",
    "            \"cache_hits\": 0,\n",
    "            \"cache_misses\": 0,\n",
    "            \"average_search_time\": 0\n",
    "        }\n",
    "        logger.info(\"Pipeline metrics reset\")\n",
    "\n",
    "# Initialize the complete RAG pipeline\n",
    "rag_pipeline = EfficientRAGPipeline(config)\n",
    "print(\"🚀 Complete RAG pipeline initialized and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35867f4",
   "metadata": {},
   "source": [
    "## 8. Interactive Query Interface and Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process documents (replace with your PDF directory path)\n",
    "# Uncomment and modify the path below to process your PDFs\n",
    "\n",
    "# DEMO_PDF_DIRECTORY = \"./sample_pdfs\"  # Replace with your PDF directory\n",
    "# success = rag_pipeline.process_documents(DEMO_PDF_DIRECTORY)\n",
    "\n",
    "# For this demo, we'll show the system status\n",
    "print(\"=== RAG Pipeline System Status ===\")\n",
    "status = rag_pipeline.get_system_status()\n",
    "\n",
    "for section, data in status.items():\n",
    "    print(f\"\\n{section.upper().replace('_', ' ')}:\")\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {data}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"📋 To process your documents, update the DEMO_PDF_DIRECTORY path above\")\n",
    "print(\"🔍 Once documents are loaded, you can query the system using the examples below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_query_response(response_data: Dict[str, Any]):\n",
    "    \"\"\"Display query response in a formatted way\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"🤖 RAG SYSTEM RESPONSE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"📝 QUERY: {response_data['query']}\")\n",
    "    print(f\"⏱️  PROCESSING TIME: {response_data.get('processing_time', 0):.3f} seconds\")\n",
    "    print(f\"📊 SEARCH RESULTS: {response_data.get('search_results_count', 0)} documents found\")\n",
    "    print(f\"🔄 RERANKING: {'Enabled' if response_data.get('reranking_enabled', False) else 'Disabled'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"💬 RESPONSE:\")\n",
    "    print(\"=\"*80)\n",
    "    print(response_data['response'])\n",
    "    \n",
    "    if response_data.get('sources'):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"📚 SOURCES:\")\n",
    "        print(\"=\"*80)\n",
    "        for i, source in enumerate(response_data['sources'], 1):\n",
    "            score = source.get('relevance_score', 'N/A')\n",
    "            print(f\"{i}. {source['policy_name']} - {source['page_number']} (Score: {score})\")\n",
    "    \n",
    "    if response_data.get('error'):\n",
    "        print(f\"\\n⚠️  ERROR: {response_data['error']}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "def query_system(query: str, enable_reranking: bool = True):\n",
    "    \"\"\"Query the RAG system and display results\"\"\"\n",
    "    response = rag_pipeline.query(query, enable_reranking=enable_reranking)\n",
    "    display_query_response(response)\n",
    "    return response\n",
    "\n",
    "# Example usage:\n",
    "print(\"🔧 Interactive Query System Ready!\")\n",
    "print(\"💡 Use: query_system('Your question here') to ask questions\")\n",
    "print(\"📖 Example queries:\")\n",
    "print(\"   - 'What are the premium rates for different age groups?'\")\n",
    "print(\"   - 'Does the policy cover pre-existing conditions?'\")\n",
    "print(\"   - 'What is the claim process for this insurance?'\")\n",
    "print(\"   - 'What are the exclusions in this policy?'\")\n",
    "\n",
    "# Uncomment to try a sample query:\n",
    "# query_system(\"What are the premium rates for different age groups?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f2fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_search_methods(query: str):\n",
    "    \"\"\"Compare semantic search vs reranked results\"\"\"\n",
    "    print(f\"🔍 COMPARING SEARCH METHODS FOR: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get search results\n",
    "    search_results = rag_pipeline.search_engine.search(query)\n",
    "    \n",
    "    if search_results.empty:\n",
    "        print(\"❌ No search results found\")\n",
    "        return\n",
    "    \n",
    "    # Compare rankings\n",
    "    comparison = rag_pipeline.reranker.compare_rankings(query, search_results)\n",
    "    \n",
    "    print(f\"📊 SEMANTIC SEARCH TOP {config.top_k_rerank}:\")\n",
    "    semantic_top = search_results.nsmallest(config.top_k_rerank, 'Distances')\n",
    "    for i, (_, row) in enumerate(semantic_top.iterrows(), 1):\n",
    "        print(f\"  {i}. Distance: {row['Distances']:.4f} | {row['Metadatas']['Policy_Name']} - {row['Metadatas']['Page_No']}\")\n",
    "    \n",
    "    print(f\"\\n🎯 RERANKED TOP {config.top_k_rerank}:\")\n",
    "    reranked = rag_pipeline.reranker.rerank_results(query, search_results)\n",
    "    reranked_top = reranked.head(config.top_k_rerank)\n",
    "    for i, (_, row) in enumerate(reranked_top.iterrows(), 1):\n",
    "        score = row.get('Rerank_Score', 'N/A')\n",
    "        print(f\"  {i}. Score: {score:.4f} | {row['Metadatas']['Policy_Name']} - {row['Metadatas']['Page_No']}\")\n",
    "    \n",
    "    print(f\"\\n📈 COMPARISON METRICS:\")\n",
    "    print(f\"  Overlap: {comparison.get('overlap_count', 0)}/{config.top_k_rerank} ({comparison.get('overlap_percentage', 0):.1f}%)\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "def benchmark_system(queries: List[str], iterations: int = 1):\n",
    "    \"\"\"Benchmark system performance with multiple queries\"\"\"\n",
    "    print(f\"⚡ BENCHMARKING SYSTEM WITH {len(queries)} QUERIES ({iterations} iterations each)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        print(f\"\\nIteration {i+1}/{iterations}\")\n",
    "        for query in queries:\n",
    "            result = rag_pipeline.query(query, enable_reranking=True)\n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'processing_time': result.get('processing_time', 0),\n",
    "                'success': 'error' not in result,\n",
    "                'search_results': result.get('search_results_count', 0)\n",
    "            })\n",
    "    \n",
    "    # Calculate statistics\n",
    "    processing_times = [r['processing_time'] for r in results]\n",
    "    success_rate = (sum(r['success'] for r in results) / len(results)) * 100\n",
    "    \n",
    "    print(f\"\\n📊 BENCHMARK RESULTS:\")\n",
    "    print(f\"  Total Queries: {len(results)}\")\n",
    "    print(f\"  Success Rate: {success_rate:.1f}%\")\n",
    "    print(f\"  Avg Processing Time: {np.mean(processing_times):.3f}s\")\n",
    "    print(f\"  Min Processing Time: {np.min(processing_times):.3f}s\")\n",
    "    print(f\"  Max Processing Time: {np.max(processing_times):.3f}s\")\n",
    "    print(f\"  Std Processing Time: {np.std(processing_times):.3f}s\")\n",
    "    \n",
    "    # Display system metrics\n",
    "    print(f\"\\n🔧 CURRENT SYSTEM METRICS:\")\n",
    "    metrics = rag_pipeline.get_system_status()\n",
    "    if 'search_metrics' in metrics:\n",
    "        search_metrics = metrics['search_metrics']\n",
    "        print(f\"  Cache Hit Rate: {search_metrics.get('cache_hit_rate', 'N/A')}\")\n",
    "        print(f\"  Total Searches: {search_metrics.get('total_searches', 0)}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Example benchmark queries\n",
    "SAMPLE_QUERIES = [\n",
    "    \"What are the premium rates?\",\n",
    "    \"What conditions are covered?\",\n",
    "    \"How do I file a claim?\",\n",
    "    \"What are the exclusions?\",\n",
    "    \"What is the waiting period?\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Benchmarking Tools Ready!\")\n",
    "print(\"💡 Usage:\")\n",
    "print(\"   - compare_search_methods('Your query') - Compare semantic vs reranked results\")\n",
    "print(\"   - benchmark_system(SAMPLE_QUERIES) - Run performance benchmark\")\n",
    "print(\"   - rag_pipeline.get_system_status() - View detailed system status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ebe58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintain_system():\n",
    "    \"\"\"Perform system maintenance tasks\"\"\"\n",
    "    print(\"🔧 PERFORMING SYSTEM MAINTENANCE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Clear caches\n",
    "    print(\"1. Clearing search cache...\")\n",
    "    rag_pipeline.search_engine.clear_cache()\n",
    "    \n",
    "    # Optimize vector database\n",
    "    print(\"2. Optimizing vector database...\")\n",
    "    try:\n",
    "        rag_pipeline.vector_db.collection.get()  # Force connection check\n",
    "        print(\"   ✅ Vector database connection verified\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Vector database issue: {e}\")\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    print(\"3. Cleaning temporary files...\")\n",
    "    import tempfile\n",
    "    import shutil\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    print(f\"   Temp directory: {temp_dir}\")\n",
    "    \n",
    "    # Check disk space\n",
    "    print(\"4. Checking disk space...\")\n",
    "    import shutil\n",
    "    total, used, free = shutil.disk_usage(config.vector_store_path)\n",
    "    print(f\"   Total: {total // (2**30)} GB\")\n",
    "    print(f\"   Used: {used // (2**30)} GB\")  \n",
    "    print(f\"   Free: {free // (2**30)} GB\")\n",
    "    \n",
    "    print(\"✅ Maintenance completed!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def export_system_config():\n",
    "    \"\"\"Export current system configuration\"\"\"\n",
    "    import json\n",
    "    \n",
    "    config_dict = {\n",
    "        'openai_model': config.openai_model,\n",
    "        'embedding_model': config.embedding_model,\n",
    "        'reranker_model': config.reranker_model,\n",
    "        'chunk_size': config.chunk_size,\n",
    "        'chunk_overlap': config.chunk_overlap,\n",
    "        'top_k_search': config.top_k_search,\n",
    "        'top_k_rerank': config.top_k_rerank,\n",
    "        'batch_size': config.batch_size,\n",
    "        'vector_store_path': config.vector_store_path,\n",
    "        'enable_caching': config.enable_caching,\n",
    "        'cache_size': config.cache_size,\n",
    "        'log_level': config.log_level,\n",
    "        'max_retries': config.max_retries,\n",
    "        'retry_delay': config.retry_delay\n",
    "    }\n",
    "    \n",
    "    config_json = json.dumps(config_dict, indent=2)\n",
    "    print(\"📝 CURRENT SYSTEM CONFIGURATION:\")\n",
    "    print(\"=\"*50)\n",
    "    print(config_json)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return config_dict\n",
    "\n",
    "def optimize_for_speed():\n",
    "    \"\"\"Optimize system for speed (may reduce accuracy)\"\"\"\n",
    "    print(\"⚡ OPTIMIZING SYSTEM FOR SPEED\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Reduce chunk overlap\n",
    "    config.chunk_overlap = 50\n",
    "    print(f\"✓ Reduced chunk overlap to {config.chunk_overlap}\")\n",
    "    \n",
    "    # Reduce search results\n",
    "    config.top_k_search = 3\n",
    "    print(f\"✓ Reduced search results to {config.top_k_search}\")\n",
    "    \n",
    "    # Disable reranking for speed\n",
    "    config.top_k_rerank = 0\n",
    "    print(\"✓ Disabled reranking for maximum speed\")\n",
    "    \n",
    "    # Increase cache size\n",
    "    config.cache_size = 200\n",
    "    print(f\"✓ Increased cache size to {config.cache_size}\")\n",
    "    \n",
    "    print(\"⚡ Speed optimization completed!\")\n",
    "    print(\"💡 Use optimize_for_accuracy() to restore accuracy settings\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "def optimize_for_accuracy():\n",
    "    \"\"\"Optimize system for accuracy (may reduce speed)\"\"\"\n",
    "    print(\"🎯 OPTIMIZING SYSTEM FOR ACCURACY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Increase chunk overlap\n",
    "    config.chunk_overlap = 200\n",
    "    print(f\"✓ Increased chunk overlap to {config.chunk_overlap}\")\n",
    "    \n",
    "    # Increase search results\n",
    "    config.top_k_search = 10\n",
    "    print(f\"✓ Increased search results to {config.top_k_search}\")\n",
    "    \n",
    "    # Enable reranking\n",
    "    config.top_k_rerank = 5\n",
    "    print(f\"✓ Enabled reranking with top {config.top_k_rerank} results\")\n",
    "    \n",
    "    print(\"🎯 Accuracy optimization completed!\")\n",
    "    print(\"💡 Use optimize_for_speed() if you need faster responses\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "print(\"🛠️ System Utilities Ready!\")\n",
    "print(\"💡 Available commands:\")\n",
    "print(\"   - maintain_system() - Perform maintenance tasks\")\n",
    "print(\"   - export_system_config() - View current configuration\")\n",
    "print(\"   - optimize_for_speed() - Optimize for faster responses\")\n",
    "print(\"   - optimize_for_accuracy() - Optimize for better accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb0833",
   "metadata": {},
   "source": [
    "## 🚀 Getting Started Guide\n",
    "\n",
    "### 1. First Time Setup\n",
    "```python\n",
    "# Step 1: Update the PDF directory path\n",
    "DEMO_PDF_DIRECTORY = r\"C:\\path\\to\\your\\insurance\\pdfs\"  # Update this path!\n",
    "\n",
    "# Step 2: Initialize the system (run this once)\n",
    "rag_pipeline = EfficientRAGPipeline()\n",
    "rag_pipeline.setup_system(DEMO_PDF_DIRECTORY)\n",
    "```\n",
    "\n",
    "### 2. Basic Usage\n",
    "```python\n",
    "# Query the system\n",
    "response = query_system(\"What are the premium rates for health insurance?\")\n",
    "\n",
    "# Compare search methods\n",
    "compare_search_methods(\"What conditions are covered?\")\n",
    "\n",
    "# Check system status\n",
    "rag_pipeline.get_system_status()\n",
    "```\n",
    "\n",
    "### 3. Performance Optimization\n",
    "```python\n",
    "# For faster responses (less accuracy)\n",
    "optimize_for_speed()\n",
    "\n",
    "# For better accuracy (slower responses)\n",
    "optimize_for_accuracy()\n",
    "\n",
    "# System maintenance\n",
    "maintain_system()\n",
    "```\n",
    "\n",
    "### 4. Troubleshooting\n",
    "- **No results found**: Check if PDFs are processed correctly\n",
    "- **Slow responses**: Try `optimize_for_speed()`\n",
    "- **Poor accuracy**: Try `optimize_for_accuracy()`\n",
    "- **Memory issues**: Reduce `batch_size` or `cache_size` in config\n",
    "- **API errors**: Check OpenAI API key and internet connection\n",
    "\n",
    "### 5. Key Features\n",
    "- ✅ **Modular Architecture**: Easy to extend and maintain\n",
    "- ✅ **Intelligent Caching**: Faster repeated queries\n",
    "- ✅ **Batch Processing**: Efficient document processing\n",
    "- ✅ **Error Handling**: Robust error recovery\n",
    "- ✅ **Performance Metrics**: Built-in monitoring\n",
    "- ✅ **Cross-encoder Reranking**: Improved relevance\n",
    "- ✅ **Configurable**: Easy to tune for your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 SYSTEM READINESS CHECK\n",
    "print(\"🔍 CHECKING SYSTEM READINESS...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if system is initialized\n",
    "try:\n",
    "    if 'rag_pipeline' in locals():\n",
    "        print(\"✅ RAG Pipeline: Initialized\")\n",
    "        \n",
    "        # Check components\n",
    "        if hasattr(rag_pipeline, 'pdf_processor'):\n",
    "            print(\"✅ PDF Processor: Ready\")\n",
    "        if hasattr(rag_pipeline, 'vector_db'):\n",
    "            print(\"✅ Vector Database: Ready\")\n",
    "        if hasattr(rag_pipeline, 'search_engine'):\n",
    "            print(\"✅ Search Engine: Ready\")\n",
    "        if hasattr(rag_pipeline, 'reranker'):\n",
    "            print(\"✅ Reranker: Ready\")\n",
    "        if hasattr(rag_pipeline, 'generator'):\n",
    "            print(\"✅ Generator: Ready\")\n",
    "            \n",
    "        # Check if documents are loaded\n",
    "        try:\n",
    "            status = rag_pipeline.get_system_status()\n",
    "            doc_count = status.get('vector_db', {}).get('document_count', 0)\n",
    "            if doc_count > 0:\n",
    "                print(f\"✅ Documents Loaded: {doc_count} chunks\")\n",
    "            else:\n",
    "                print(\"⚠️ No documents loaded yet\")\n",
    "        except:\n",
    "            print(\"⚠️ Document status unknown\")\n",
    "            \n",
    "    else:\n",
    "        print(\"❌ RAG Pipeline: Not initialized\")\n",
    "        print(\"💡 Run the initialization cells above first!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error checking system: {e}\")\n",
    "\n",
    "print(\"\\n🎯 NEXT STEPS:\")\n",
    "if 'rag_pipeline' not in locals():\n",
    "    print(\"1. ⬆️ Run all cells above to initialize the system\")\n",
    "    print(\"2. 📝 Update DEMO_PDF_DIRECTORY with your PDF path\")\n",
    "    print(\"3. 🏃 Run rag_pipeline.setup_system(DEMO_PDF_DIRECTORY)\")\n",
    "elif 'doc_count' in locals() and doc_count == 0:\n",
    "    print(\"1. 📝 Update DEMO_PDF_DIRECTORY with your PDF path\")\n",
    "    print(\"2. 🏃 Run rag_pipeline.setup_system(DEMO_PDF_DIRECTORY)\")\n",
    "else:\n",
    "    print(\"1. 🚀 Start querying: query_system('Your question here')\")\n",
    "    print(\"2. 📊 Compare methods: compare_search_methods('Your query')\")\n",
    "    print(\"3. ⚙️ Optimize: optimize_for_speed() or optimize_for_accuracy()\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
