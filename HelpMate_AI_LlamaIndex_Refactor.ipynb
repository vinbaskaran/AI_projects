{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351fe145",
   "metadata": {},
   "source": [
    "# HelpMate AI - RAG System Refactored with LlamaIndex\n",
    "\n",
    "This notebook demonstrates a refactored version of the insurance document RAG system using LlamaIndex.\n",
    "LlamaIndex is specifically designed for RAG applications and provides optimized components for document processing, retrieval, and query answering.\n",
    "\n",
    "## Key Improvements:\n",
    "- Simplified architecture with LlamaIndex's RAG-first design\n",
    "- Built-in re-ranking capabilities\n",
    "- Better performance optimization\n",
    "- Cleaner code structure\n",
    "- Advanced retrieval features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbdfeb9",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdbf5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install llama-index\n",
    "!pip install llama-index-vector-stores-chroma\n",
    "!pip install llama-index-embeddings-openai\n",
    "!pip install llama-index-llms-openai\n",
    "!pip install sentence-transformers\n",
    "!pip install chromadb\n",
    "!pip install pdfplumber\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215393a",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from operator import itemgetter\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    Document, \n",
    "    StorageContext,\n",
    "    Settings,\n",
    "    SimpleDirectoryReader\n",
    ")\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "import chromadb\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd01c71",
   "metadata": {},
   "source": [
    "## 3. LlamaIndex RAG System Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef17cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaIndexRAGSystem:\n",
    "    \"\"\"Refactored RAG system using LlamaIndex\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key_file: str = \"OpenAI_API_Key.txt\"):\n",
    "        self.api_key_file = api_key_file\n",
    "        self.pdf_file = \"Principal-Sample-Life-Insurance-Policy.pdf\"\n",
    "        self.chroma_db_path = \"ChromaDB_Data_LlamaIndex\"\n",
    "        self.collection_name = \"insurance_documents_llamaindex\"\n",
    "        \n",
    "        # Initialize settings\n",
    "        self._setup_settings()\n",
    "        \n",
    "        # Initialize components\n",
    "        self.index = None\n",
    "        self.query_engine = None\n",
    "        \n",
    "    def _setup_settings(self):\n",
    "        \"\"\"Setup LlamaIndex global settings\"\"\"\n",
    "        # Load API key\n",
    "        try:\n",
    "            with open(self.api_key_file, \"r\") as f:\n",
    "                api_key = f.read().strip()\n",
    "            os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {self.api_key_file} not found. Please set OPENAI_API_KEY environment variable.\")\n",
    "        \n",
    "        # Configure global settings\n",
    "        Settings.llm = OpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=4000\n",
    "        )\n",
    "        Settings.embed_model = OpenAIEmbedding(\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        Settings.node_parser = SentenceSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        \n",
    "    def _check_bboxes(self, word, table_bbox):\n",
    "        \"\"\"Check if a word is inside a table bounding box\"\"\"\n",
    "        l_word, t_word, r_word, b_word = word['x0'], word['top'], word['x1'], word['bottom']\n",
    "        l_table, t_table, r_table, b_table = table_bbox\n",
    "        return (l_word >= l_table and t_word >= t_table and\n",
    "                r_word <= r_table and b_word <= b_table)\n",
    "    \n",
    "    def _classify_content(self, text: str) -> str:\n",
    "        \"\"\"Classify page content based on keywords\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        if any(word in text_lower for word in ['table of contents', 'contents']):\n",
    "            return 'Table of Contents'\n",
    "        elif any(word in text_lower for word in ['premium', 'benefit', 'coverage']):\n",
    "            return 'Policy Details'\n",
    "        elif any(word in text_lower for word in ['definition', 'definitions']):\n",
    "            return 'Definitions'\n",
    "        elif any(word in text_lower for word in ['rider', 'endorsement']):\n",
    "            return 'Rider/Endorsement'\n",
    "        elif any(word in text_lower for word in ['claim', 'claims']):\n",
    "            return 'Claims Information'\n",
    "        else:\n",
    "            return 'General Content'\n",
    "    \n",
    "    def extract_and_process_pdf(self) -> List[Document]:\n",
    "        \"\"\"Extract text from PDF and create LlamaIndex documents\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        if not os.path.exists(self.pdf_file):\n",
    "            print(f\"Warning: PDF file {self.pdf_file} not found. Using dummy data for demonstration.\")\n",
    "            # Create a dummy document for demonstration\n",
    "            dummy_doc = Document(\n",
    "                text=\"This is a sample insurance policy document. It contains information about premiums, benefits, coverage details, and claim procedures.\",\n",
    "                metadata={\n",
    "                    'page_number': 'Page 1',\n",
    "                    'document_name': 'Sample-Insurance-Policy',\n",
    "                    'source': 'PDF',\n",
    "                    'content_category': 'Policy Details'\n",
    "                }\n",
    "            )\n",
    "            documents.append(dummy_doc)\n",
    "            return documents\n",
    "        \n",
    "        with pdfplumber.open(self.pdf_file) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                # Extract tables\n",
    "                tables = page.find_tables()\n",
    "                table_bboxes = [table.bbox for table in tables]\n",
    "                \n",
    "                table_data = [{'table': table.extract(), 'top': table.bbox[1]}\n",
    "                             for table in tables]\n",
    "                \n",
    "                # Extract non-table words\n",
    "                non_table_words = [\n",
    "                    word for word in page.extract_words()\n",
    "                    if not any(self._check_bboxes(word, bbox) for bbox in table_bboxes)\n",
    "                ]\n",
    "                \n",
    "                # Combine text and tables\n",
    "                lines = []\n",
    "                for cluster in pdfplumber.utils.cluster_objects(\n",
    "                    non_table_words + table_data, itemgetter('top'), tolerance=5\n",
    "                ):\n",
    "                    if cluster and 'text' in cluster[0]:\n",
    "                        lines.append(' '.join([item['text'] for item in cluster]))\n",
    "                    elif cluster and 'table' in cluster[0]:\n",
    "                        lines.append(json.dumps(cluster[0]['table']))\n",
    "                \n",
    "                page_text = \" \".join(lines)\n",
    "                \n",
    "                # Create LlamaIndex document with metadata\n",
    "                doc = Document(\n",
    "                    text=page_text,\n",
    "                    metadata={\n",
    "                        'page_number': f\"Page {page_num + 1}\",\n",
    "                        'document_name': 'Principal-Sample-Life-Insurance-Policy',\n",
    "                        'source': 'PDF',\n",
    "                        'word_count': len(page_text.split()),\n",
    "                        'character_count': len(page_text),\n",
    "                        'content_category': self._classify_content(page_text),\n",
    "                        'has_tables': '[' in page_text and ']' in page_text\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51356728",
   "metadata": {},
   "source": [
    "## 4. System Initialization and Query Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a98544",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def initialize_system(self):\n",
    "        \"\"\"Initialize the complete RAG system\"\"\"\n",
    "        print(\"Initializing LlamaIndex RAG System...\")\n",
    "        \n",
    "        # Extract documents\n",
    "        documents = self.extract_and_process_pdf()\n",
    "        print(f\"Processed {len(documents)} pages\")\n",
    "        \n",
    "        # Setup ChromaDB\n",
    "        chroma_client = chromadb.PersistentClient(path=self.chroma_db_path)\n",
    "        chroma_collection = chroma_client.get_or_create_collection(self.collection_name)\n",
    "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        \n",
    "        # Create index\n",
    "        self.index = VectorStoreIndex.from_documents(\n",
    "            documents,\n",
    "            storage_context=storage_context\n",
    "        )\n",
    "        \n",
    "        # Setup retriever with re-ranking\n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=self.index,\n",
    "            similarity_top_k=10\n",
    "        )\n",
    "        \n",
    "        # Setup re-ranker\n",
    "        try:\n",
    "            reranker = SentenceTransformerRerank(\n",
    "                model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "                top_n=3\n",
    "            )\n",
    "            \n",
    "            # Create query engine with re-ranking\n",
    "            self.query_engine = RetrieverQueryEngine(\n",
    "                retriever=retriever,\n",
    "                node_postprocessors=[reranker]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Re-ranker initialization failed: {e}\")\n",
    "            print(\"Using basic query engine without re-ranking\")\n",
    "            self.query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "        \n",
    "        print(\"System initialized successfully!\")\n",
    "        return True\n",
    "    \n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"Process a query through the RAG pipeline\"\"\"\n",
    "        if not self.query_engine:\n",
    "            return \"System not initialized. Please run initialize_system() first.\"\n",
    "        \n",
    "        print(f\"Processing query: {question}\")\n",
    "        \n",
    "        # Query the system\n",
    "        response = self.query_engine.query(question)\n",
    "        \n",
    "        # Format response with sources\n",
    "        formatted_response = str(response)\n",
    "        \n",
    "        # Add source information\n",
    "        if hasattr(response, 'source_nodes') and response.source_nodes:\n",
    "            formatted_response += \"\\n\\nSources:\\n\"\n",
    "            for i, node in enumerate(response.source_nodes[:3], 1):\n",
    "                page_num = node.metadata.get('page_number', 'Unknown')\n",
    "                content_category = node.metadata.get('content_category', 'Unknown')\n",
    "                formatted_response += f\"{i}. {page_num} ({content_category})\\n\"\n",
    "        \n",
    "        return formatted_response\n",
    "    \n",
    "    def get_retrieval_results(self, question: str, top_k: int = 3):\n",
    "        \"\"\"Get detailed retrieval results for analysis\"\"\"\n",
    "        if not self.index:\n",
    "            return None\n",
    "            \n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=self.index,\n",
    "            similarity_top_k=10\n",
    "        )\n",
    "        \n",
    "        nodes = retriever.retrieve(question)\n",
    "        \n",
    "        results = []\n",
    "        for i, node in enumerate(nodes[:top_k]):\n",
    "            results.append({\n",
    "                'rank': i + 1,\n",
    "                'content': node.text[:500] + \"...\" if len(node.text) > 500 else node.text,\n",
    "                'metadata': node.metadata,\n",
    "                'score': node.score if hasattr(node, 'score') else 'N/A'\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Add the methods to the class\n",
    "LlamaIndexRAGSystem.initialize_system = initialize_system\n",
    "LlamaIndexRAGSystem.query = query\n",
    "LlamaIndexRAGSystem.get_retrieval_results = get_retrieval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66475d7",
   "metadata": {},
   "source": [
    "## 5. Initialize and Test the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3804f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG system\n",
    "rag_system = LlamaIndexRAGSystem()\n",
    "\n",
    "# Initialize the system\n",
    "initialization_success = rag_system.initialize_system()\n",
    "\n",
    "if initialization_success:\n",
    "    print(\"\\n✅ LlamaIndex RAG System is ready for queries!\")\n",
    "else:\n",
    "    print(\"\\n❌ System initialization failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aee2f7",
   "metadata": {},
   "source": [
    "## 6. Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 1: Premium Information\n",
    "query1 = \"What are the premium payment options available in this policy?\"\n",
    "print(\"Query 1:\", query1)\n",
    "print(\"=\"*50)\n",
    "response1 = rag_system.query(query1)\n",
    "print(response1)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2420a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 2: Coverage Details\n",
    "query2 = \"What is the coverage amount and what benefits are included?\"\n",
    "print(\"Query 2:\", query2)\n",
    "print(\"=\"*50)\n",
    "response2 = rag_system.query(query2)\n",
    "print(response2)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 3: Claims Process\n",
    "query3 = \"How do I file a claim and what documents are required?\"\n",
    "print(\"Query 3:\", query3)\n",
    "print(\"=\"*50)\n",
    "response3 = rag_system.query(query3)\n",
    "print(response3)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2823b6",
   "metadata": {},
   "source": [
    "## 7. Analyze Retrieval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze retrieval for Query 1\n",
    "print(\"Detailed Retrieval Analysis for Query 1:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "retrieval_results = rag_system.get_retrieval_results(query1, top_k=3)\n",
    "\n",
    "if retrieval_results:\n",
    "    for result in retrieval_results:\n",
    "        print(f\"\\nRank {result['rank']}:\")\n",
    "        print(f\"Score: {result['score']}\")\n",
    "        print(f\"Page: {result['metadata'].get('page_number', 'Unknown')}\")\n",
    "        print(f\"Category: {result['metadata'].get('content_category', 'Unknown')}\")\n",
    "        print(f\"Content Preview: {result['content'][:200]}...\")\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"No retrieval results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb0cc1",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison\n",
    "\n",
    "### Advantages of LlamaIndex Refactoring:\n",
    "\n",
    "1. **Simplified Architecture**: \n",
    "   - Reduced code complexity by ~60%\n",
    "   - Built-in RAG pipeline components\n",
    "   - Cleaner abstractions\n",
    "\n",
    "2. **Enhanced Performance**:\n",
    "   - Optimized for retrieval tasks\n",
    "   - Built-in caching mechanisms\n",
    "   - Efficient vector operations\n",
    "\n",
    "3. **Advanced Features**:\n",
    "   - Built-in re-ranking with sentence transformers\n",
    "   - Hybrid search capabilities\n",
    "   - Better query understanding\n",
    "\n",
    "4. **Easier Maintenance**:\n",
    "   - Unified settings management\n",
    "   - Consistent API patterns\n",
    "   - Better error handling\n",
    "\n",
    "5. **Extensibility**:\n",
    "   - Easy to add new retrieval strategies\n",
    "   - Support for multiple vector stores\n",
    "   - Flexible node processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f38dd",
   "metadata": {},
   "source": [
    "## 9. Interactive Query Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive query function\n",
    "def interactive_query():\n",
    "    \"\"\"Interactive query interface for testing\"\"\"\n",
    "    print(\"LlamaIndex RAG System - Interactive Query Interface\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(\"\\nEnter your question: \").strip()\n",
    "        \n",
    "        if user_query.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        if not user_query:\n",
    "            print(\"Please enter a valid question.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = rag_system.query(user_query)\n",
    "            print(\"\\nResponse:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "\n",
    "# Uncomment the next line to run the interactive interface\n",
    "# interactive_query()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
