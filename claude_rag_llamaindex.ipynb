{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8ef9e1",
   "metadata": {},
   "source": [
    "# Production-Grade RAG System for Insurance Document Analysis\n",
    "Optimized Version with Enhanced Performance and Modularity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80683a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from functools import lru_cache\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6fc29",
   "metadata": {},
   "source": [
    "## CONFIGURATION MANAGEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bcb4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Centralized configuration for RAG system parameters\"\"\"\n",
    "    \n",
    "    # Model configuration\n",
    "    model_name: str = 'gpt-3.5-turbo'\n",
    "    temperature: float = 0.7\n",
    "    \n",
    "    # Retrieval parameters\n",
    "    chunk_size: int = 512\n",
    "    chunk_overlap: int = 50\n",
    "    similarity_top_k: int = 5\n",
    "    \n",
    "    # Content filtering thresholds\n",
    "    min_content_length: int = 100\n",
    "    min_content_indicators: int = 1\n",
    "    \n",
    "    # Confidence scoring parameters\n",
    "    max_source_score: int = 25\n",
    "    max_length_score: int = 20\n",
    "    max_specificity_score: int = 25\n",
    "    max_uncertainty_penalty: int = 20\n",
    "    max_precision_score: int = 15\n",
    "    max_source_quality_score: int = 20\n",
    "    \n",
    "    # Content quality multipliers\n",
    "    severe_penalty_multiplier: float = 0.01\n",
    "    moderate_penalty_multiplier: float = 0.3\n",
    "    content_boost_multiplier: float = 1.5\n",
    "    \n",
    "    # Response parameters\n",
    "    optimal_response_length_min: int = 30\n",
    "    optimal_response_length_max: int = 150\n",
    "    context_window_size: int = 4\n",
    "    \n",
    "    # Performance settings\n",
    "    enable_caching: bool = True\n",
    "    cache_ttl: int = 3600  # seconds\n",
    "    max_retries: int = 3\n",
    "    timeout: int = 30  # seconds\n",
    "\n",
    "class QuestionType(Enum):\n",
    "    \"\"\"Enumeration of question types for classification\"\"\"\n",
    "    FACTUAL = \"factual\"\n",
    "    COMPARISON = \"comparison\"\n",
    "    PROCEDURAL = \"procedural\"\n",
    "    SUMMARY = \"summary\"\n",
    "    FOLLOWUP = \"followup\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52cb719",
   "metadata": {},
   "source": [
    "## CONTENT QUALITY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentQualityAnalyzer:\n",
    "    \"\"\"Optimized content quality assessment and filtering\"\"\"\n",
    "    \n",
    "    # Pre-compiled patterns for better performance\n",
    "    SEVERE_PENALTY_PATTERNS = re.compile(\n",
    "        r'table of contents|gc 6001 table of contents|'\n",
    "        r'this policy has been updated effective january 1, 2014 gc 6001',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    MODERATE_PENALTY_PATTERNS = re.compile(\n",
    "        r'section [a-d] -|part [iv]+ -|page \\d{1,2}(?!\\d)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    CONTENT_BOOST_PATTERNS = re.compile(\n",
    "        r'coverage exclusion|claim procedure|premium payment|'\n",
    "        r'death benefit|proof of loss|notice of claim|'\n",
    "        r'medical examination|autopsy|legal action',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    CONTENT_INDICATORS = {\n",
    "        'coverage', 'benefit', 'exclusion', 'procedure', 'payment',\n",
    "        'claim', 'premium', 'death', 'accident', 'medical',\n",
    "        'within', 'days', 'shall', 'must', 'required', 'employee',\n",
    "        'insurance', 'policy', 'amount', 'termination', 'effective'\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    @lru_cache(maxsize=1024)\n",
    "    def has_severe_penalty(cls, text: str) -> bool:\n",
    "        \"\"\"Check if content should receive severe penalty (cached)\"\"\"\n",
    "        return bool(cls.SEVERE_PENALTY_PATTERNS.search(text))\n",
    "    \n",
    "    @classmethod\n",
    "    @lru_cache(maxsize=1024)\n",
    "    def has_moderate_penalty(cls, text: str) -> bool:\n",
    "        \"\"\"Check if content should receive moderate penalty (cached)\"\"\"\n",
    "        if len(text) >= 300:\n",
    "            return False\n",
    "        return bool(cls.MODERATE_PENALTY_PATTERNS.search(text))\n",
    "    \n",
    "    @classmethod\n",
    "    @lru_cache(maxsize=1024)\n",
    "    def should_boost_content(cls, text: str, query: str) -> bool:\n",
    "        \"\"\"Determine if content should be boosted (cached)\"\"\"\n",
    "        relevant_topics = {'exclusion', 'procedure', 'payment', 'claim'}\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        if not any(topic in query_lower for topic in relevant_topics):\n",
    "            return False\n",
    "        \n",
    "        return bool(cls.CONTENT_BOOST_PATTERNS.search(text))\n",
    "    \n",
    "    @classmethod\n",
    "    def count_content_indicators(cls, text: str) -> int:\n",
    "        \"\"\"Count content quality indicators efficiently\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        text_words = set(text_lower.split())\n",
    "        return len(cls.CONTENT_INDICATORS & text_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473aa8f",
   "metadata": {},
   "source": [
    "## OPTIMIZED RETRIEVERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedBM25Retriever:\n",
    "    \"\"\"Performance-optimized BM25 retriever with intelligent content boosting\"\"\"\n",
    "    \n",
    "    def __init__(self, nodes, config: RAGConfig):\n",
    "        self.nodes = nodes\n",
    "        self.config = config\n",
    "        self.analyzer = ContentQualityAnalyzer()\n",
    "        \n",
    "        # Pre-tokenize and cache for performance\n",
    "        self.tokenized_docs = [node.text.lower().split() for node in nodes]\n",
    "        \n",
    "        # Initialize BM25\n",
    "        from rank_bm25 import BM25Okapi\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "        \n",
    "        # Pre-compute node text hashes for caching\n",
    "        self.node_text_cache = {i: node.text.lower() for i, node in enumerate(nodes)}\n",
    "    \n",
    "    def retrieve(self, query_str: str) -> List:\n",
    "        \"\"\"Retrieve nodes with optimized content quality boosting\"\"\"\n",
    "        query_text = self._extract_query_text(query_str)\n",
    "        tokenized_query = query_text.lower().split()\n",
    "        \n",
    "        # Get BM25 scores\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Apply quality boosting using vectorized operations\n",
    "        boosted_scores = self._boost_content_quality_vectorized(scores, query_text)\n",
    "        \n",
    "        # Get top results efficiently\n",
    "        top_indices = np.argpartition(boosted_scores, -self.config.similarity_top_k)[-self.config.similarity_top_k:]\n",
    "        top_indices = top_indices[np.argsort(boosted_scores[top_indices])][::-1]\n",
    "        \n",
    "        # Return results with positive scores\n",
    "        from llama_index.core.schema import NodeWithScore\n",
    "        return [\n",
    "            NodeWithScore(node=self.nodes[i], score=boosted_scores[i])\n",
    "            for i in top_indices if boosted_scores[i] > 0\n",
    "        ]\n",
    "    \n",
    "    def _boost_content_quality_vectorized(self, scores: np.ndarray, query_text: str) -> np.ndarray:\n",
    "        \"\"\"Vectorized content quality boosting for performance\"\"\"\n",
    "        boosted_scores = scores.copy()\n",
    "        \n",
    "        for i, cached_text in self.node_text_cache.items():\n",
    "            if self.analyzer.has_severe_penalty(cached_text):\n",
    "                boosted_scores[i] *= self.config.severe_penalty_multiplier\n",
    "            elif self.analyzer.has_moderate_penalty(cached_text):\n",
    "                boosted_scores[i] *= self.config.moderate_penalty_multiplier\n",
    "            elif self.analyzer.should_boost_content(cached_text, query_text):\n",
    "                boosted_scores[i] *= self.config.content_boost_multiplier\n",
    "        \n",
    "        return boosted_scores\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_query_text(query_str) -> str:\n",
    "        \"\"\"Extract text from various query formats\"\"\"\n",
    "        if hasattr(query_str, 'query_str'):\n",
    "            return query_str.query_str\n",
    "        elif hasattr(query_str, 'text'):\n",
    "            return query_str.text\n",
    "        return str(query_str)\n",
    "\n",
    "class OptimizedHybridRetriever:\n",
    "    \"\"\"High-performance hybrid retriever with intelligent filtering\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_retriever, bm25_retriever, config: RAGConfig):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.config = config\n",
    "        self.analyzer = ContentQualityAnalyzer()\n",
    "        \n",
    "        # Cache for filtering results\n",
    "        self._filter_cache = {}\n",
    "    \n",
    "    def retrieve(self, query_str: str) -> List:\n",
    "        \"\"\"Retrieve and intelligently filter results\"\"\"\n",
    "        query_text = self._extract_query_text(query_str)\n",
    "        \n",
    "        # Parallel retrieval (can be optimized with threading)\n",
    "        vector_results = self.vector_retriever.retrieve(query_text)\n",
    "        bm25_results = self.bm25_retriever.retrieve(query_text)\n",
    "        \n",
    "        # Combine and deduplicate\n",
    "        filtered_results = self._filter_and_deduplicate_optimized(\n",
    "            vector_results + bm25_results\n",
    "        )\n",
    "        \n",
    "        # Apply selective backup if needed\n",
    "        if len(filtered_results) < 2:\n",
    "            filtered_results = self._apply_selective_backup(\n",
    "                vector_results + bm25_results, filtered_results\n",
    "            )\n",
    "        \n",
    "        return filtered_results[:self.config.similarity_top_k]\n",
    "    \n",
    "    def _filter_and_deduplicate_optimized(self, all_results: List) -> List:\n",
    "        \"\"\"Optimized filtering and deduplication using set operations\"\"\"\n",
    "        seen_texts = set()\n",
    "        filtered_results = []\n",
    "        \n",
    "        for result in all_results:\n",
    "            text_hash = hash(result.node.text)\n",
    "            \n",
    "            if text_hash not in seen_texts and self._is_substantial_content(result.node):\n",
    "                seen_texts.add(text_hash)\n",
    "                filtered_results.append(result)\n",
    "        \n",
    "        return filtered_results\n",
    "    \n",
    "    @lru_cache(maxsize=512)\n",
    "    def _is_substantial_content(self, node) -> bool:\n",
    "        \"\"\"Cached content quality assessment\"\"\"\n",
    "        text = node.text.lower().strip()\n",
    "        \n",
    "        # Quick rejection checks\n",
    "        if self.analyzer.has_severe_penalty(text) or len(text) < self.config.min_content_length:\n",
    "            return False\n",
    "        \n",
    "        # Medium-length structural content check\n",
    "        if len(text) < 200 and self.analyzer.has_moderate_penalty(text):\n",
    "            return False\n",
    "        \n",
    "        # Content indicator requirement\n",
    "        return self.analyzer.count_content_indicators(text) >= self.config.min_content_indicators\n",
    "    \n",
    "    def _apply_selective_backup(self, all_results: List, current_results: List) -> List:\n",
    "        \"\"\"Apply intelligent backup mechanism\"\"\"\n",
    "        seen_texts = {hash(result.node.text) for result in current_results}\n",
    "        \n",
    "        for result in all_results:\n",
    "            if (hash(result.node.text) not in seen_texts and \n",
    "                len(current_results) < self.config.similarity_top_k and\n",
    "                self._is_acceptable_backup(result.node)):\n",
    "                current_results.append(result)\n",
    "                seen_texts.add(hash(result.node.text))\n",
    "        \n",
    "        return current_results\n",
    "    \n",
    "    def _is_acceptable_backup(self, node) -> bool:\n",
    "        \"\"\"Determine if content is acceptable as backup\"\"\"\n",
    "        text = node.text.lower().strip()\n",
    "        \n",
    "        if 'table of contents' in text or len(text) < 80:\n",
    "            return False\n",
    "        \n",
    "        policy_terms = {'coverage', 'benefit', 'claim', 'insurance', 'policy', 'employee', 'procedure'}\n",
    "        return any(word in text for word in policy_terms)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_query_text(query_str) -> str:\n",
    "        \"\"\"Extract text from various query formats\"\"\"\n",
    "        if hasattr(query_str, 'query_str'):\n",
    "            return query_str.query_str\n",
    "        elif hasattr(query_str, 'text'):\n",
    "            return query_str.text\n",
    "        return str(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17a7d7",
   "metadata": {},
   "source": [
    "## INTELLIGENT QUERY CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryClassifier:\n",
    "    \"\"\"Optimized query classification with caching\"\"\"\n",
    "    \n",
    "    CLASSIFICATION_RULES = {\n",
    "        QuestionType.FACTUAL: {\n",
    "            'keywords': frozenset(['what', 'who', 'when', 'where', 'which']),\n",
    "            'pattern': re.compile(r'\\b(what|who|when|where|which)\\b', re.IGNORECASE)\n",
    "        },\n",
    "        QuestionType.COMPARISON: {\n",
    "            'keywords': frozenset(['compare', 'difference', 'vs', 'versus', 'better']),\n",
    "            'pattern': re.compile(r'\\b(compare|difference|vs|versus|better)\\b', re.IGNORECASE)\n",
    "        },\n",
    "        QuestionType.PROCEDURAL: {\n",
    "            'keywords': frozenset(['how', 'process', 'procedure', 'steps']),\n",
    "            'pattern': re.compile(r'\\b(how|process|procedure|steps)\\b', re.IGNORECASE)\n",
    "        },\n",
    "        QuestionType.SUMMARY: {\n",
    "            'keywords': frozenset(['summarize', 'summary', 'overview', 'explain']),\n",
    "            'pattern': re.compile(r'\\b(summarize|summary|overview|explain)\\b', re.IGNORECASE)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    FOLLOWUP_INDICATORS = frozenset([\n",
    "        'elaborate', 'explain more', 'tell me more', 'expand', 'details',\n",
    "        'that', 'it', 'this', 'further', 'more about', 'specific',\n",
    "        'can you', 'what about', 'how about'\n",
    "    ])\n",
    "    \n",
    "    @classmethod\n",
    "    @lru_cache(maxsize=256)\n",
    "    def classify_question(cls, question: str) -> QuestionType:\n",
    "        \"\"\"Classify question type with caching\"\"\"\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # Check for follow-up first\n",
    "        if any(indicator in question_lower for indicator in cls.FOLLOWUP_INDICATORS):\n",
    "            return QuestionType.FOLLOWUP\n",
    "        \n",
    "        # Check classification patterns\n",
    "        for q_type, rules in cls.CLASSIFICATION_RULES.items():\n",
    "            if rules['pattern'].search(question_lower):\n",
    "                return q_type\n",
    "        \n",
    "        return QuestionType.FACTUAL\n",
    "    \n",
    "    @classmethod\n",
    "    def should_use_sub_questions(cls, question: str, question_type: QuestionType) -> bool:\n",
    "        \"\"\"Determine if sub-question engine should be used\"\"\"\n",
    "        if question_type in [QuestionType.COMPARISON, QuestionType.SUMMARY]:\n",
    "            return True\n",
    "        \n",
    "        return len(question.split()) > 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b41c7",
   "metadata": {},
   "source": [
    "## ADVANCED CONFIDENCE SCORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c393df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfidenceScorer:\n",
    "    \"\"\"Optimized multi-factor confidence scoring system\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        # Pre-compile patterns for performance\n",
    "        self.specific_indicators = re.compile(\n",
    "            r'\\b(section|page|part|according to|states that|specifically|'\n",
    "            r'outlined|policy|coverage|benefit|procedure|days|within)\\b',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        self.uncertainty_patterns = re.compile(\n",
    "            r'\\b(not sure|unclear|might be|possibly|perhaps|generally|'\n",
    "            r'typically|usually|contact the|consult with|it is advisable)\\b',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        self.number_pattern = re.compile(r'\\d+')\n",
    "    \n",
    "    def calculate_confidence_score(self, response: str, retrieved_nodes: List) -> Tuple[int, List[str]]:\n",
    "        \"\"\"Calculate comprehensive confidence score\"\"\"\n",
    "        score = 0.0\n",
    "        factors = []\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        # Factor assessments\n",
    "        assessments = [\n",
    "            self._assess_source_quantity(retrieved_nodes),\n",
    "            self._assess_response_length(response),\n",
    "            self._assess_policy_specificity(response_lower),\n",
    "            self._assess_uncertainty(response_lower),\n",
    "            self._assess_numerical_precision(response),\n",
    "            self._assess_source_quality(retrieved_nodes)\n",
    "        ]\n",
    "        \n",
    "        # Aggregate scores\n",
    "        for assessment_score, factor_desc in assessments:\n",
    "            if assessment_score != 0:\n",
    "                score += assessment_score\n",
    "                factors.append(factor_desc)\n",
    "        \n",
    "        # Normalize and add variability\n",
    "        final_score = max(0, min(100, score + random.uniform(-3, 3)))\n",
    "        \n",
    "        return round(final_score), factors\n",
    "    \n",
    "    def _assess_source_quantity(self, retrieved_nodes: List) -> Tuple[float, str]:\n",
    "        \"\"\"Assess score based on number of supporting sources\"\"\"\n",
    "        num_sources = len(retrieved_nodes) if retrieved_nodes else 0\n",
    "        source_score = min(num_sources * 5, self.config.max_source_score)\n",
    "        return source_score, f\"Sources: {num_sources} (+{source_score}pts)\"\n",
    "    \n",
    "    def _assess_response_length(self, response: str) -> Tuple[float, str]:\n",
    "        \"\"\"Assess response quality based on length\"\"\"\n",
    "        word_count = len(response.split())\n",
    "        \n",
    "        if self.config.optimal_response_length_min <= word_count <= self.config.optimal_response_length_max:\n",
    "            score = 20\n",
    "        elif 20 <= word_count < self.config.optimal_response_length_min or \\\n",
    "             self.config.optimal_response_length_max < word_count <= 200:\n",
    "            score = 15\n",
    "        elif 10 <= word_count < 20 or 200 < word_count <= 300:\n",
    "            score = 10\n",
    "        else:\n",
    "            score = 5\n",
    "        \n",
    "        return score, f\"Length: {word_count} words (+{score}pts)\"\n",
    "    \n",
    "    def _assess_policy_specificity(self, response_lower: str) -> Tuple[float, str]:\n",
    "        \"\"\"Assess specificity of policy references\"\"\"\n",
    "        matches = len(self.specific_indicators.findall(response_lower))\n",
    "        score = min(matches * 3, self.config.max_specificity_score)\n",
    "        return score, f\"Policy specificity: {matches} terms (+{score}pts)\"\n",
    "    \n",
    "    def _assess_uncertainty(self, response_lower: str) -> Tuple[float, str]:\n",
    "        \"\"\"Detect and penalize uncertain language\"\"\"\n",
    "        matches = len(self.uncertainty_patterns.findall(response_lower))\n",
    "        penalty = min(matches * 8, self.config.max_uncertainty_penalty)\n",
    "        return -penalty if penalty > 0 else 0, f\"Uncertainty: -{penalty}pts\" if penalty > 0 else \"\"\n",
    "    \n",
    "    def _assess_numerical_precision(self, response: str) -> Tuple[float, str]:\n",
    "        \"\"\"Assess numerical precision\"\"\"\n",
    "        numbers = len(self.number_pattern.findall(response))\n",
    "        score = min(numbers * 3, self.config.max_precision_score)\n",
    "        return score if score > 0 else 0, f\"Numerical precision: {numbers} values (+{score}pts)\" if score > 0 else \"\"\n",
    "    \n",
    "    def _assess_source_quality(self, retrieved_nodes: List) -> Tuple[float, str]:\n",
    "        \"\"\"Enhanced source quality assessment\"\"\"\n",
    "        if not retrieved_nodes:\n",
    "            return -5, \"Source quality: No sources (-5pts)\"\n",
    "        \n",
    "        substantial_sources = sum(\n",
    "            1 for node in retrieved_nodes \n",
    "            if len(node.node.text) > 150\n",
    "        )\n",
    "        \n",
    "        quality_score = min(substantial_sources * 4, 16)\n",
    "        \n",
    "        if quality_score > 0:\n",
    "            return quality_score, f\"Source quality: {substantial_sources} substantial (+{quality_score}pts)\"\n",
    "        else:\n",
    "            return -5, \"Source quality: Low-quality sources (-5pts)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fe1ce",
   "metadata": {},
   "source": [
    "## PERFORMANCE MONITORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QueryMetrics:\n",
    "    \"\"\"Data class for query performance metrics\"\"\"\n",
    "    timestamp: str\n",
    "    question: str\n",
    "    question_type: QuestionType\n",
    "    processing_time: float\n",
    "    confidence_score: int\n",
    "    num_sources: int\n",
    "    response_length: int\n",
    "    context_used: bool\n",
    "    sub_questions_used: bool\n",
    "    confidence_factors: List[str]\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Optimized performance monitoring system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.query_history: List[QueryMetrics] = []\n",
    "        self.metrics_cache = {\n",
    "            'total_queries': 0,\n",
    "            'avg_processing_time': 0.0,\n",
    "            'avg_confidence_score': 0.0,\n",
    "            'question_type_distribution': {},\n",
    "            'source_quality_stats': {}\n",
    "        }\n",
    "    \n",
    "    def log_query(self, metrics: QueryMetrics):\n",
    "        \"\"\"Log query performance metrics\"\"\"\n",
    "        self.query_history.append(metrics)\n",
    "        self._update_metrics(metrics)\n",
    "    \n",
    "    def _update_metrics(self, metrics: QueryMetrics):\n",
    "        \"\"\"Update aggregate metrics efficiently\"\"\"\n",
    "        self.metrics_cache['total_queries'] += 1\n",
    "        n = self.metrics_cache['total_queries']\n",
    "        \n",
    "        # Update running averages\n",
    "        self.metrics_cache['avg_processing_time'] = (\n",
    "            (self.metrics_cache['avg_processing_time'] * (n - 1) + metrics.processing_time) / n\n",
    "        )\n",
    "        self.metrics_cache['avg_confidence_score'] = (\n",
    "            (self.metrics_cache['avg_confidence_score'] * (n - 1) + metrics.confidence_score) / n\n",
    "        )\n",
    "        \n",
    "        # Update distributions\n",
    "        q_type = metrics.question_type.value\n",
    "        self.metrics_cache['question_type_distribution'][q_type] = \\\n",
    "            self.metrics_cache['question_type_distribution'].get(q_type, 0) + 1\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance summary\"\"\"\n",
    "        return {\n",
    "            'metrics': self.metrics_cache,\n",
    "            'recent_queries': [\n",
    "                {\n",
    "                    'question': q.question[:60],\n",
    "                    'type': q.question_type.value,\n",
    "                    'time': f\"{q.processing_time:.2f}s\",\n",
    "                    'confidence': q.confidence_score\n",
    "                }\n",
    "                for q in self.query_history[-5:]\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667ceca",
   "metadata": {},
   "source": [
    "## MAIN RAG SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092699f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRAGSystem:\n",
    "    \"\"\"Main RAG system with all optimizations integrated\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig = None):\n",
    "        self.config = config or RAGConfig()\n",
    "        self.performance_monitor = PerformanceMonitor()\n",
    "        self.query_classifier = QueryClassifier()\n",
    "        self.confidence_scorer = ConfidenceScorer(self.config)\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # Initialize components (placeholders for actual initialization)\n",
    "        self.hybrid_retriever = None\n",
    "        self.query_engine = None\n",
    "        self.sub_question_engine = None\n",
    "        \n",
    "        logger.info(\"RAG System initialized with optimized configuration\")\n",
    "    \n",
    "    def initialize_components(self, documents, llm):\n",
    "        \"\"\"Initialize all RAG components\"\"\"\n",
    "        from llama_index.core import VectorStoreIndex\n",
    "        from llama_index.core.node_parser import SentenceSplitter\n",
    "        from llama_index.core.retrievers import VectorIndexRetriever\n",
    "        from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "        from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "        \n",
    "        # Parse documents\n",
    "        parser = SentenceSplitter(\n",
    "            chunk_size=self.config.chunk_size,\n",
    "            chunk_overlap=self.config.chunk_overlap\n",
    "        )\n",
    "        nodes = parser.get_nodes_from_documents(documents)\n",
    "        \n",
    "        # Build index\n",
    "        index = VectorStoreIndex(nodes)\n",
    "        \n",
    "        # Create retrievers\n",
    "        vector_retriever = VectorIndexRetriever(\n",
    "            index=index,\n",
    "            similarity_top_k=self.config.similarity_top_k\n",
    "        )\n",
    "        bm25_retriever = OptimizedBM25Retriever(nodes, self.config)\n",
    "        self.hybrid_retriever = OptimizedHybridRetriever(\n",
    "            vector_retriever, bm25_retriever, self.config\n",
    "        )\n",
    "        \n",
    "        # Create query engine\n",
    "        self.query_engine = RetrieverQueryEngine(\n",
    "            retriever=self.hybrid_retriever,\n",
    "            response_synthesizer=get_response_synthesizer(response_mode=\"compact\")\n",
    "        )\n",
    "        \n",
    "        # Try to create sub-question engine\n",
    "        try:\n",
    "            from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "            from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "            \n",
    "            tools = [\n",
    "                QueryEngineTool(\n",
    "                    query_engine=self.query_engine,\n",
    "                    metadata=ToolMetadata(\n",
    "                        name=\"insurance_policy\",\n",
    "                        description=\"Insurance policy information\"\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "            self.sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
    "                query_engine_tools=tools,\n",
    "                llm=llm\n",
    "            )\n",
    "        except ImportError:\n",
    "            logger.warning(\"SubQuestionQueryEngine not available, using standard engine\")\n",
    "            self.sub_question_engine = self.query_engine\n",
    "        \n",
    "        logger.info(\"All components initialized successfully\")\n",
    "    \n",
    "    def process_query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a query with full optimization\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Classify question\n",
    "        question_type = self.query_classifier.classify_question(question)\n",
    "        \n",
    "        # Build contextual question\n",
    "        contextual_question = self._build_contextual_question(question, question_type)\n",
    "        \n",
    "        # Select appropriate engine\n",
    "        if self.query_classifier.should_use_sub_questions(question, question_type):\n",
    "            response = self.sub_question_engine.query(contextual_question)\n",
    "        else:\n",
    "            response = self.query_engine.query(contextual_question)\n",
    "        \n",
    "        # Calculate confidence\n",
    "        source_nodes = getattr(response, 'source_nodes', [])\n",
    "        confidence, factors = self.confidence_scorer.calculate_confidence_score(\n",
    "            response.response, source_nodes\n",
    "        )\n",
    "        \n",
    "        # Log performance\n",
    "        processing_time = time.time() - start_time\n",
    "        metrics = QueryMetrics(\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            question=question,\n",
    "            question_type=question_type,\n",
    "            processing_time=processing_time,\n",
    "            confidence_score=confidence,\n",
    "            num_sources=len(source_nodes),\n",
    "            response_length=len(response.response.split()),\n",
    "            context_used=len(self.conversation_history) > 0,\n",
    "            sub_questions_used=self.query_classifier.should_use_sub_questions(question, question_type),\n",
    "            confidence_factors=factors\n",
    "        )\n",
    "        self.performance_monitor.log_query(metrics)\n",
    "        \n",
    "        # Update conversation history\n",
    "        self.conversation_history.append({'role': 'user', 'content': question})\n",
    "        self.conversation_history.append({'role': 'assistant', 'content': response.response})\n",
    "        \n",
    "        # Maintain context window\n",
    "        if len(self.conversation_history) > self.config.context_window_size * 2:\n",
    "            self.conversation_history = self.conversation_history[-(self.config.context_window_size * 2):]\n",
    "        \n",
    "        return {\n",
    "            'response': response.response,\n",
    "            'question_type': question_type.value,\n",
    "            'confidence': confidence,\n",
    "            'factors': factors,\n",
    "            'processing_time': processing_time,\n",
    "            'source_nodes': source_nodes\n",
    "        }\n",
    "    \n",
    "    def _build_contextual_question(self, question: str, question_type: QuestionType) -> str:\n",
    "        \"\"\"Build contextual question with conversation history\"\"\"\n",
    "        if question_type == QuestionType.FOLLOWUP and self.conversation_history:\n",
    "            # Build follow-up context\n",
    "            recent_history = self.conversation_history[-4:]\n",
    "            context = \"\\n\".join([\n",
    "                f\"{msg['role'].title()}: {msg['content'][:200]}\"\n",
    "                for msg in recent_history\n",
    "            ])\n",
    "            return f\"Context:\\n{context}\\n\\nFollow-up Question: {question}\"\n",
    "        \n",
    "        elif self.conversation_history:\n",
    "            # Regular context\n",
    "            recent_context = self.conversation_history[-2]['content'][:200] if len(self.conversation_history) >= 2 else \"\"\n",
    "            return f\"Previous context: {recent_context}\\n\\nNew Question: {question}\"\n",
    "        \n",
    "        return question\n",
    "    \n",
    "    def reset_conversation(self):\n",
    "        \"\"\"Reset conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        logger.info(\"Conversation history reset\")\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get system performance summary\"\"\"\n",
    "        return self.performance_monitor.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f70f2",
   "metadata": {},
   "source": [
    "## MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50542646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Initialize configuration\n",
    "    config = RAGConfig()\n",
    "    \n",
    "    # Initialize system\n",
    "    rag_system = OptimizedRAGSystem(config)\n",
    "    \n",
    "    # Load documents and initialize components\n",
    "    # (This would be done with actual document loading)\n",
    "    # rag_system.initialize_components(documents, llm)\n",
    "    \n",
    "    logger.info(\"Optimized RAG System ready for use\")\n",
    "    \n",
    "    return rag_system\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    system = main()\n",
    "    print(\"RAG System initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f60f92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
